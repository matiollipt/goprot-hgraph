{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene Ontology Graph Preprocessing\n",
    "Author: Cleverson Matiolli, Ph.D.\n",
    "\n",
    "**Objective:** This notebook focuses on preprocessing the **Gene Ontology Directed Acyclic graph (GO DAG)** for use in the Aid2GO Heterogeneous Graph Attention Network (Aid2GO-HAN).\n",
    "\n",
    "**Key Steps:**\n",
    "1. Download and parse the Gene Ontology (GO) directed acyclic graph (DAG)\n",
    "2. Calculate Information Content (IC)\n",
    "3. Extract relationships between GO terms\n",
    "4. Extract nodes (GO terms) and nodes' features (***not implemented***)\n",
    "5. Embed GO term definitions\n",
    "6. Evaluate the quality of GO terms' embeddings\n",
    "7. Build the GO Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Bioinformatics\n",
    "import networkx as nx\n",
    "from obonet import read_obo\n",
    "\n",
    "# Machine learning \n",
    "import torch\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Custom libraries\n",
    "import aid2go.go as aidgo\n",
    "import aid2go.ia as aidia\n",
    "import aid2go.utils as aidutils\n",
    " \n",
    "# Configuration\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and parse the Gene Ontology (GO) directed acyclic graph (DAG)\n",
    "\n",
    "Permanent link: http://purl.obolibrary.org/obo/go/go-basic.obo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_dag, go_df = aidgo.get_godag(\"./data/go/\", timeout=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Information Content (IC)\n",
    "\n",
    "Information accretion (IA), introduced in [Clark and Radivojac, 2013], is a measure of how much information is added to an ontology annotation by node $v$ given that its parents $Pa(v)$ are already annotated. Specifically,\n",
    "\n",
    "$$\n",
    "ia(v) = \\log_2 \\frac{1}{\\Pr(v | Pa(v))} = \\log_2 \\frac{\\Pr(Pa(v))}{\\Pr(Pa(v) | v) \\Pr(v)}\n",
    "$$\n",
    "\n",
    ">*De Paolis Kaluza, C. (2024). Information Accretion (Version 1.0.0) [Computer software]. GitHub. https://github.com/claradepaolis/InformationAccretion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter GO DAG Edges (\"is_a\", \"part_of\")\n",
    "\n",
    "- Before propagating annotations, we filter out all edges that are not in the scope of the evaluation of predicted annotations (*\"regulates\", \"negatively_regulates\", \"positively-regulates\"*, **BPO subontology**). CAFA evaluators consider only \"is_a\" and \"part_of\" edges for the GO DAG complete/consistent subgraph corresponding to the annotated GO term\n",
    "\n",
    "Filter-out BPO exclusive relations , keeping only *\"is_a\"* and *\"part_of\"* relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_dag_cleaned = aidia.clean_ontology_edges(go_dag)\n",
    "print(f\"# edges original GO DAG: {go_dag.number_of_edges()}\")\n",
    "print(f\"# edges cleaned GO DAG: {go_dag_cleaned.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GOA Annotation File\n",
    "\n",
    "The GOA high-confidence (hc) annotations encompasses only protein-GO term associations from experimental evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations file (GOA high-confidence)\n",
    "annot_df = pd.read_csv(\"./data/goa/goa_hc_annot.tsv\", sep=\"\\t\")\n",
    "annot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagate Annotations\n",
    "\n",
    "Genes annotated under a GO term inherit the annotation from all its ancestors, with the annotation of the term corresponding to node $v$ necessarily implying the annotation of terms $\\mathcal{Pa}(v)$ to guarantee a *consistent subgraph* (valid annotations).\n",
    "\n",
    "- The propagation of annotations follow the direction of edges: *(head --(\"is_a\", \"part_of\")--> tail)*, i.e., *children --(\"is_a\", \"part_of\") --> parents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the three subontologies\n",
    "roots = {\"BPO\": \"GO:0008150\", \"CCO\": \"GO:0005575\", \"MFO\": \"GO:0003674\"}\n",
    "subontologies = {\n",
    "    aspect: aidia.fetch_aspect(go_dag_cleaned, roots[aspect]) for aspect in roots\n",
    "}\n",
    "\n",
    "print(\"BPO -->\", len(subontologies[\"BPO\"]))\n",
    "print(\"MFO -->\", len(subontologies[\"MFO\"]))\n",
    "print(\"CCO -->\", len(subontologies[\"CCO\"]))\n",
    "\n",
    "# Propagate terms\n",
    "annot_df_prop = aidia.propagate_terms(annot_df, subontologies)\n",
    "\n",
    "# Save propagated terms\n",
    "annot_df_prop.to_csv(\n",
    "    \"./data/goa/goa_hc_annot_prop.tsv\",\n",
    "    # header=False,\n",
    "    index=False,\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "print(f\"annotations df propagated terms shape: {annot_df_prop.shape}\")\n",
    "annot_df_prop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Aspects and Terms Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count term instances\n",
    "print(\"Counting Terms\")\n",
    "aspect_counts = dict()\n",
    "aspect_terms = dict()\n",
    "term_idx = dict()\n",
    "for aspect, subont in subontologies.items():\n",
    "    aspect_terms[aspect] = sorted(subont.nodes)  # ensure same order\n",
    "    term_idx[aspect] = {t: i for i, t in enumerate(aspect_terms[aspect])}\n",
    "    aspect_counts[aspect] = aidia.term_counts(\n",
    "        annot_df_prop[annot_df_prop.aspect == aspect], term_idx[aspect]\n",
    "    )\n",
    "\n",
    "    assert aspect_counts[aspect].sum() == len(\n",
    "        annot_df_prop[annot_df_prop.aspect == aspect]\n",
    "    ) + len(aspect_terms[aspect])\n",
    "    \n",
    "print(f\"Length: Aspect Counts -> {len(aspect_counts)}\")\n",
    "print(f\"Length: Aspect Terms -> {len(aspect_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are indexing by column to compute IA, \n",
    "# let's convert to Compressed Sparse Column format\n",
    "sp_matrix = {aspect:dok.tocsc() for aspect, dok in aspect_counts.items()}\n",
    "\n",
    "# Compute IA\n",
    "print('Computing Information Accretion')\n",
    "aspect_ia = {aspect: {t:0 for t in aspect_terms[aspect]} for aspect in aspect_terms.keys()}\n",
    "for aspect, subontology in subontologies.items():\n",
    "    for term in aspect_ia[aspect].keys():\n",
    "        aspect_ia[aspect][term] = aidia.calc_ia(term, sp_matrix[aspect], subontology, term_idx[aspect])\n",
    "\n",
    "ia_df = pd.concat([pd.DataFrame.from_dict(\n",
    "    {'term':aspect_ia[aspect].keys(), \n",
    "        'ia': aspect_ia[aspect].values(), \n",
    "        'aspect': aspect}) for aspect in subontologies.keys()])\n",
    "\n",
    "# All counts should be non-negative\n",
    "assert ia_df['ia'].min() >= 0\n",
    "\n",
    "# Save to file\n",
    "ia_df[['term','ia']].to_csv(\"./data/go/ia.txt\",  header=None, sep='\\t', index=False)\n",
    "print(ia_df.shape)\n",
    "ia_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map IC Values to GO Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations file\n",
    "go_df = pd.read_csv(\"./data/go/go-basic.csv\")\n",
    "\n",
    "# Merge annotations and IC values\n",
    "go_df = pd.merge(go_df, ia_df, left_on=\"go_id\", right_on=\"term\", how=\"left\")\n",
    "go_df.drop(columns=[\"namespace\"], inplace=True)  # Drop redundant subontology column\n",
    "\n",
    "# Reorder columns for easy visualization\n",
    "go_df = go_df[\n",
    "    [\n",
    "            \"go_id\",\n",
    "            \"name\",\n",
    "            \"aspect\",\n",
    "            \"definition\",\n",
    "            \"def_word_count\",\n",
    "            \"in_degree\",\n",
    "            \"out_degree\",\n",
    "            \"degree\",\n",
    "            \"term\",\n",
    "            \"ia\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Save updated GO df\n",
    "go_df.reset_index(drop=True, inplace=True)\n",
    "go_df.to_csv(\"./data/go/go-basic.csv\", index=False)\n",
    "print(go_df.shape)\n",
    "go_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Relationships Between GO Terms\n",
    "\n",
    "In the Gene Ontology (GO) graph, relations are encoded using five types: \"is_a\" for subclass relationships, \"part_of\" for part-whole relationships, \"regulates,\" \"positively_regulates,\" and \"negatively_regulates\" for various types of regulation between terms (https://wiki.geneontology.org/index.php/Relation_composition).\n",
    "\n",
    "1. Main relationships (BPO, MFO, CCO): \"is_a\", \"part_of\" **(used to calculate IC**)\n",
    "2. Other relationships (BPO only): \"regulates\", \"positively_regulates\", \"negatively_regulates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_edges_df = aidgo.get_go_relations(go_dag, \"./data/go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_edges_df_cleaned = aidgo.get_go_relations(go_dag_cleaned, \"./data/go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Nodes/Edges Features\n",
    "\n",
    "1. Node degree\n",
    "2. Graph Centrality measures\n",
    "3. Information Content (IC)\n",
    "4. GO term namespace (BPO, MFO, CCO)\n",
    "5. Local subgraph properties (clustering coefficients, graphlets)\n",
    "6. Hierarchical position (ontological structure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embed GO Terms' Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings of GO definitions\n",
    "\n",
    "# Create directory to save embeddings\n",
    "save_path = Path(\"./data/go\")\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "go_embed_dict, id_text_mapping = aidgo.embed_texts(\n",
    "    df=go_df,\n",
    "    column_id=\"go_id\",\n",
    "    column_text=\"definition\",\n",
    "    batch_size=8,\n",
    "    pre_trained_model=\"dmis-lab/biobert-v1.1\",\n",
    "    save_path=save_path,\n",
    ")\n",
    "\n",
    "print(f\"Length of GO embeddings dict: {len(go_embed_dict)}\")\n",
    "print(f\"Length of ID mapping dict: {len(id_text_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the quality of GO terms' embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Embeddings of Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GO embeddings\n",
    "filepath =  \"./data/go/go_emb_dict-definition.pkl\"\n",
    "with open(filepath, \"rb\") as file:\n",
    "    go_embed_dict_def = pickle.load(file)\n",
    "\n",
    "print(f\"Length: {len(go_embed_dict_def)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract a small subset of GO embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract GO terms and embeddings\n",
    "go_terms = np.array(list(go_embed_dict_def.keys()))\n",
    "embeddings = np.array(list(go_embed_dict_def.values()))\n",
    "\n",
    "# Sample embeddings\n",
    "ids_sample, embeddings_sample = resample(\n",
    "    go_terms,\n",
    "    embeddings,\n",
    "    n_samples=5000,\n",
    "    replace=False,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Create term -> aspect mapping\n",
    "for aspect, subont in subontologies.items():\n",
    "    aspect_terms[aspect] = sorted(subont.nodes)  # ensure same order\n",
    "\n",
    "aspect_terms = {aspect: sorted(subont.nodes) for aspect, subont in subontologies.items()}\n",
    "\n",
    "print(f\"sample ids shape: {ids_sample.shape}, embeddings shape: {embeddings_sample.shape},\")\n",
    "print(f\"aspect terms dict keys: {aspect_terms.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reduce dimensionality using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=8, random_state=42)\n",
    "reduced_embeddings = pca.fit_transform(embeddings_sample)\n",
    "\n",
    "print(f\"Reduced embeddings shape: {reduced_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimate Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampled embeddings\n",
    "num_of_neighbors = [5, 10, 25, 50, 75, 100]\n",
    "\n",
    "# Path to save plots\n",
    "save_path = Path(\"./eda/go/hdbscan\")\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results = aidgo.analyze_clusters(\n",
    "    reduced_embeddings,\n",
    "    ids_sample,\n",
    "    num_of_neighbors,\n",
    "    save_path,\n",
    "    0.5,\n",
    "    show_plot=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Clusters and Silhouette Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect Namespaces Frequencies in Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_go_terms_to_namespace(go_terms, aspect_terms):\n",
    "    \"\"\"\n",
    "    Map GO terms to their corresponding namespaces using the aspect_terms dictionary.\n",
    "\n",
    "    :param go_terms: List of GO terms.\n",
    "    :param aspect_terms: Dictionary with keys as namespaces ('BPO', 'MFO', 'CCO')\n",
    "                         and values as sets of GO terms corresponding to each namespace.\n",
    "    :return: A numpy array where each element is the namespace ('BP', 'MF', 'CC')\n",
    "             corresponding to the GO term.\n",
    "    \"\"\"\n",
    "    namespace_data = []\n",
    "    for go_term in go_terms:\n",
    "        if go_term in aspect_terms[\"BPO\"]:\n",
    "            namespace_data.append(\"BP\")  # Biological Process\n",
    "        elif go_term in aspect_terms[\"MFO\"]:\n",
    "            namespace_data.append(\"MF\")  # Molecular Function\n",
    "        elif go_term in aspect_terms[\"CCO\"]:\n",
    "            namespace_data.append(\"CC\")  # Cellular Component\n",
    "        else:\n",
    "            namespace_data.append(\"Unknown\")  # For terms not found\n",
    "\n",
    "    return np.array(namespace_data)\n",
    "\n",
    "\n",
    "def plot_combined_bar_charts(\n",
    "    clustered_counts, noisy_counts, stacked_data, clusters, namespaces, color_map\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a plot with three subplots: stacked bar chart for namespaces in clusters,\n",
    "    and two bar charts for clustered and noisy namespace frequencies. Annotate bars with counts.\n",
    "\n",
    "    :param clustered_counts: Counter object with frequencies of namespaces in clustered data.\n",
    "    :param noisy_counts: Counter object with frequencies of namespaces in noisy data.\n",
    "    :param stacked_data: Data for the stacked bar chart.\n",
    "    :param clusters: Array of unique clusters.\n",
    "    :param namespaces: List of namespaces ('BP', 'MF', 'CC').\n",
    "    :param color_map: Dictionary mapping namespaces to specific colors.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    # Stacked bar chart for namespaces in each cluster\n",
    "    bottom = np.zeros(len(clusters))\n",
    "    for i, ns in enumerate(namespaces):\n",
    "        bars = ax[0].bar(\n",
    "            clusters, stacked_data[:, i], bottom=bottom, label=ns, color=color_map[ns]\n",
    "        )\n",
    "        bottom += stacked_data[:, i]\n",
    "\n",
    "        # # Annotate with counts\n",
    "        # for j, bar in enumerate(bars):\n",
    "        #     height = bar.get_height()\n",
    "        #     ax[0].annotate(\n",
    "        #         f\"{int(height)}\",\n",
    "        #         xy=(bar.get_x() + bar.get_width() / 2, bottom[j] - height / 2),\n",
    "        #         xytext=(0, 3),  # 3 points vertical offset\n",
    "        #         textcoords=\"offset points\",\n",
    "        #         ha=\"center\",\n",
    "        #         va=\"bottom\",\n",
    "        #     )\n",
    "\n",
    "    ax[0].set_xlabel(\"Cluster\")\n",
    "    ax[0].set_ylabel(\"Counts\")\n",
    "    ax[0].set_title(\"Counts of GO Namespaces in Each Cluster\")\n",
    "    ax[0].legend(title=\"Namespace\")\n",
    "\n",
    "    # Bar chart for clustered namespace frequencies\n",
    "    ax[1].bar(\n",
    "        clustered_counts.keys(),\n",
    "        clustered_counts.values(),\n",
    "        color=[color_map[ns] for ns in clustered_counts.keys()],\n",
    "    )\n",
    "    ax[1].set_title(\"Clustered Namespaces Frequencies\")\n",
    "    ax[1].set_ylabel(\"Count\")\n",
    "\n",
    "    # Annotate clustered bars with counts\n",
    "    for i, (label, count) in enumerate(clustered_counts.items()):\n",
    "        ax[1].text(i, count, str(count), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # Bar chart for noisy namespace frequencies\n",
    "    ax[2].bar(\n",
    "        noisy_counts.keys(),\n",
    "        noisy_counts.values(),\n",
    "        color=[color_map[ns] for ns in noisy_counts.keys()],\n",
    "    )\n",
    "    ax[2].set_title(\"Noisy Namespaces Frequencies\")\n",
    "    ax[2].set_ylabel(\"Count\")\n",
    "\n",
    "    # Annotate noisy bars with counts\n",
    "    for i, (label, count) in enumerate(noisy_counts.items()):\n",
    "        ax[2].text(i, count, str(count), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./eda/go/hdbscan/combined_clustered_namespaces_counts.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Fit HDBSCAN clusterer\n",
    "clusterer = HDBSCAN(min_cluster_size=50)\n",
    "cluster_labels = clusterer.fit_predict(reduced_embeddings)\n",
    "\n",
    "# Filter noisy data points\n",
    "filtered_indices = cluster_labels != -1\n",
    "filtered_cluster_labels = cluster_labels[filtered_indices]\n",
    "\n",
    "print(\n",
    "    f\"Unique clusters ({len(np.unique(cluster_labels))}): {np.unique(cluster_labels)}\"\n",
    ")\n",
    "\n",
    "# Assuming go_terms (or ids_sample) is the array of GO terms corresponding to the embeddings\n",
    "namespace_data = map_go_terms_to_namespace(ids_sample, aspect_terms)\n",
    "\n",
    "# Separate clustered and noisy data\n",
    "filtered_namespace_data = namespace_data[filtered_indices]\n",
    "noisy_namespace_data = namespace_data[~filtered_indices]\n",
    "\n",
    "# Count namespace frequencies for clustered and noisy data\n",
    "clustered_counts = Counter(filtered_namespace_data)\n",
    "noisy_counts = Counter(noisy_namespace_data)\n",
    "\n",
    "# Combine filtered cluster labels with namespaces\n",
    "cluster_namespace_pairs = [\n",
    "    (cluster, ns)\n",
    "    for cluster, ns in zip(filtered_cluster_labels, filtered_namespace_data)\n",
    "]\n",
    "\n",
    "# Count occurrences for each cluster and namespace\n",
    "cluster_namespace_dict = defaultdict(lambda: defaultdict(int))\n",
    "for cluster, ns in cluster_namespace_pairs:\n",
    "    cluster_namespace_dict[cluster][ns] += 1\n",
    "\n",
    "# Get unique clusters and namespaces\n",
    "clusters = np.unique(filtered_cluster_labels)\n",
    "namespaces = [\n",
    "    \"BP\",\n",
    "    \"MF\",\n",
    "    \"CC\",\n",
    "]  # Biological Process, Molecular Function, Cellular Component\n",
    "\n",
    "# Define consistent colors for each namespace\n",
    "color_map = {\"BP\": \"green\", \"MF\": \"blue\", \"CC\": \"orange\"}\n",
    "\n",
    "# Prepare the data for the stacked bar plot\n",
    "stacked_data = np.array(\n",
    "    [[cluster_namespace_dict[cluster][ns] for ns in namespaces] for cluster in clusters]\n",
    ")\n",
    "\n",
    "# Plot the combined chart with three subplots\n",
    "plot_combined_bar_charts(\n",
    "    clustered_counts, noisy_counts, stacked_data, clusters, namespaces, color_map\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_misclassified_go_terms_df(\n",
    "    cluster_labels, go_terms, namespaces, aspect_terms\n",
    "):\n",
    "    \"\"\"\n",
    "    Inspect misclassified GO terms by comparing their cluster assignments to their true namespaces.\n",
    "\n",
    "    :param cluster_labels: Array of assigned cluster labels for each GO term.\n",
    "    :param go_terms: List of GO terms corresponding to the embeddings.\n",
    "    :param namespaces: List of possible namespaces ('BP', 'MF', 'CC').\n",
    "    :param aspect_terms: Dictionary mapping namespaces to their GO terms.\n",
    "    :return: A DataFrame of misclassified GO terms.\n",
    "    \"\"\"\n",
    "    misclassified_data = []\n",
    "\n",
    "    # Map each GO term to its true namespace\n",
    "    true_namespaces = []\n",
    "    for go_term in go_terms:\n",
    "        if go_term in aspect_terms[\"BPO\"]:\n",
    "            true_namespaces.append(\"BP\")\n",
    "        elif go_term in aspect_terms[\"MFO\"]:\n",
    "            true_namespaces.append(\"MF\")\n",
    "        elif go_term in aspect_terms[\"CCO\"]:\n",
    "            true_namespaces.append(\"CC\")\n",
    "        else:\n",
    "            true_namespaces.append(\"Unknown\")  # For terms that are not found\n",
    "\n",
    "    # Loop over each unique cluster to inspect terms\n",
    "    for cluster in np.unique(cluster_labels):\n",
    "        print(f\"\\nInspecting Cluster {cluster}...\")\n",
    "\n",
    "        # Extract GO terms in the current cluster\n",
    "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "        cluster_go_terms = [go_terms[i] for i in cluster_indices]\n",
    "        cluster_namespaces = [true_namespaces[i] for i in cluster_indices]\n",
    "\n",
    "        # Count occurrences of each namespace in the current cluster\n",
    "        namespace_counts = Counter(cluster_namespaces)\n",
    "        print(f\"Namespace distribution in Cluster {cluster}: {namespace_counts}\")\n",
    "\n",
    "        # Determine the majority namespace in the cluster\n",
    "        majority_namespace = max(namespace_counts, key=namespace_counts.get)\n",
    "\n",
    "        # Find misclassified GO terms (terms that do not belong to the majority namespace)\n",
    "        for i, term in enumerate(cluster_go_terms):\n",
    "            if cluster_namespaces[i] != majority_namespace:\n",
    "                misclassified_data.append(\n",
    "                    {\n",
    "                        \"GO_term\": term,\n",
    "                        \"True_Namespace\": cluster_namespaces[i],\n",
    "                        \"Assigned_Cluster\": cluster,\n",
    "                        \"Majority_Namespace\": majority_namespace,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Convert the misclassified data to a DataFrame\n",
    "    misclassified_df = pd.DataFrame(misclassified_data)\n",
    "    return misclassified_df\n",
    "\n",
    "\n",
    "# Call the function to inspect misclassified GO terms and get the DataFrame\n",
    "misclassified_go_terms_df = inspect_misclassified_go_terms_df(\n",
    "    cluster_labels=cluster_labels,\n",
    "    go_terms=ids_sample,  # Assuming ids_sample corresponds to GO terms\n",
    "    namespaces=[\"BP\", \"MF\", \"CC\"],\n",
    "    aspect_terms=aspect_terms,  # Dictionary mapping namespaces to GO terms\n",
    ")\n",
    "\n",
    "# Print the DataFrame of misclassified GO terms\n",
    "print(\"\\nMisclassified GO terms DataFrame:\")\n",
    "misclassified_go_terms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_df[go_df[\"go_id\"].isin(misclassified_go_terms_df[\"GO_term\"])].sort_values(by=\"definition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of perplexities and lr (default is \"auto\")\n",
    "perplexities = [5, 10, 25, 50, 75, 100, 125, 150, 200]\n",
    "# learning_rate = 10\n",
    "\n",
    "# store embeddings\n",
    "tsne_dict = {}\n",
    "\n",
    "# Perform t-SNE for each perplexity value\n",
    "for perplexity in tqdm(perplexities, \"Performing t-SNE\", len(perplexities)):\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=perplexity,\n",
    "        random_state=42,\n",
    "        max_iter=300,\n",
    "        # learning_rate=learning_rate,\n",
    "    )\n",
    "    tsne_dict[perplexity] = tsne.fit_transform(reduced_embeddings)\n",
    "\n",
    "# Save plots\n",
    "save_path = Path(\"./eda/go/tsne\")\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save T-SNE embeddings\n",
    "with open(save_path / \"tsne2D_dict.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tsne_dict, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(tsne_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode namespace labels\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(namespace_data)\n",
    "\n",
    "# Create subplots for each perplexity value\n",
    "nrows, ncols = aidutils.square_layout(len(perplexities))\n",
    "fig, ax = plt.subplots(nrows, ncols, figsize=(24, 15))\n",
    "\n",
    "for i, perplexity in enumerate(perplexities):\n",
    "    tsne_results = tsne_dict[perplexity]\n",
    "\n",
    "    scatter = ax[i // 3, i % 3].scatter(\n",
    "        tsne_results[:, 0],\n",
    "        tsne_results[:, 1],\n",
    "        c=encoded_labels,\n",
    "        cmap=\"viridis\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    ax[i // 3, i % 3].set_title(f\"t-SNE Perplexity: {perplexity}\")\n",
    "\n",
    "# Create a legend\n",
    "handles, _ = scatter.legend_elements(prop=\"colors\")\n",
    "labels = le.classes_\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"GO Namespaces\",\n",
    "    loc=\"upper right\",\n",
    "    fontsize=14,\n",
    ")\n",
    "plt.savefig(save_path / \"tsne_namespaces.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode namespace labels\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(cluster_labels)\n",
    "\n",
    "# Create subplots for each perplexity value\n",
    "nrows, ncols = aidutils.square_layout(len(perplexities))\n",
    "fig, ax = plt.subplots(nrows, ncols, figsize=(24, 15))\n",
    "\n",
    "for i, perplexity in enumerate(perplexities):\n",
    "    tsne_results = tsne_dict[perplexity]\n",
    "\n",
    "    scatter = ax[i // 3, i % 3].scatter(\n",
    "        tsne_results[:, 0],\n",
    "        tsne_results[:, 1],\n",
    "        c=clusterer.labels_,\n",
    "        cmap=\"viridis\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    ax[i // 3, i % 3].set_title(f\"t-SNE Perplexity: {perplexity}\")\n",
    "\n",
    "# Create a legend\n",
    "handles, _ = scatter.legend_elements(prop=\"colors\")\n",
    "labels = le.classes_\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"HDBSCAN Clusters\",\n",
    "    loc=\"upper right\",\n",
    "    fontsize=14,\n",
    ")\n",
    "\n",
    "plt.savefig(save_path / \"tsne_cluster_labels.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build the GO Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GO node embeddings\n",
    "save_path = Path(\"./data/go\")\n",
    "filepath = save_path / \"go_emb_dict-definition.pkl\"\n",
    "\n",
    "with open(filepath, \"rb\") as file:\n",
    "    go_embed_dict = pickle.load(file)\n",
    "\n",
    "print(f\"GO embeddings dict length: {len(go_embed_dict)}\")\n",
    "for key, value in go_embed_dict.items():\n",
    "    print(\n",
    "        f\"GO id: {key}, embedding ({value.dtype}): {value[:10]}, shape: {value.shape}\"\n",
    "    )\n",
    "    break\n",
    "\n",
    "go_ids = list(go_embed_dict.keys())\n",
    "print(f\"Length GO ids list: {len(go_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edges\n",
    "\n",
    "go_edges_df = pd.read_csv(\"./data/go/go_edges.tsv\", sep=\"\\t\")\n",
    "go_edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(\"./data/go/\")\n",
    "\n",
    "# Heterogeneous\n",
    "heterodata = aidgo.create_go_data(\n",
    "    go_edges_df=go_edges_df,\n",
    "    go_embed_dict=go_embed_dict,\n",
    "    save_path=save_path,\n",
    "    multi_edge=True,\n",
    ")\n",
    "\n",
    "# Homogeneous\n",
    "data = aidgo.create_go_data(\n",
    "    go_edges_df=go_edges_df,\n",
    "    go_embed_dict=go_embed_dict,\n",
    "    save_path=save_path,\n",
    "    multi_edge=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aid2go",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
