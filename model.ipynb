{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aid2GO Heterogeneous Graph Attention Network (HAN) Training\n",
    "Author: Cleverson Matiolli, Ph.D.\n",
    "\n",
    "**Implement**\n",
    "1. F-measure with weighted precision and recall ($weight = ic(term)$)\n",
    "2. Differentiate between different GO edges (relationships)\n",
    "3. Skip-connections to mitigate loss of hierarchical information\n",
    "4. Embeddings of relation definition as edge features to model relations of GO definitions accounting for how the definitions of two connected nodes are related (maybe...)\n",
    "\n",
    "**Debug**\n",
    "1. Model predict the same GO term many times for a given protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "# Machine Learning and Deep Learning\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import (\n",
    "    to_undirected,\n",
    "    structured_negative_sampling_feasible,\n",
    "    remove_self_loops,\n",
    "    add_self_loops,\n",
    "    softmax,\n",
    "    degree,\n",
    ")\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Disable warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set plot params\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "# Set wandb notebook name environment variable\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"model.ipynb\"\n",
    "\n",
    "# Set CUDA and PyTorch environment\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"TORCH\"] = torch.__version__\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Define folder paths\n",
    "base_dir = Path(Path.cwd())\n",
    "\n",
    "output_dir = base_dir / \"outputs\"  # Model outputs\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein-GO Dataloader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinGoDataloader:\n",
    "    def __init__(self, protein_ids_map, go_ids_map, ppi_folder):\n",
    "        \"\"\"\n",
    "        Initialize the ProteinGODataset class.\n",
    "\n",
    "        Parameters:\n",
    "        protein_ids_map (dict): A dictionary mapping protein IDs to their indices.\n",
    "        go_ids_map (dict): A dictionary mapping GO IDs to their indices.\n",
    "        ppi_folder (str): The path to the folder containing the species-specific data.\n",
    "\n",
    "        Attributes:\n",
    "        protein_ids_map (dict): A dictionary mapping protein IDs to their indices.\n",
    "        go_ids_map (dict): A dictionary mapping GO IDs to their indices.\n",
    "        ppi_folder (Path): The path to the folder containing the species-specific data.\n",
    "        edge_index_protgo_positive (torch.Tensor): A tensor representing the positive edges between proteins and GO terms.\n",
    "        edge_index_protgo_negative (torch.Tensor): A tensor representing the negative edges between proteins and GO terms.\n",
    "        associations_positive_df (pandas.DataFrame): A DataFrame containing the positive associations between proteins and GO terms.\n",
    "        associations_negative_df (pandas.DataFrame): A DataFrame containing the negative associations between proteins and GO terms.\n",
    "        associations_combined_df (pandas.DataFrame): A DataFrame containing both positive and negative associations between proteins and GO terms.\n",
    "        hetero_data (torch_geometric.data.HeteroData): A heterogeneous graph data object.\n",
    "        train_mask (torch.Tensor): A tensor representing the mask for the training set.\n",
    "        val_mask (torch.Tensor): A tensor representing the mask for the validation set.\n",
    "        test_mask (torch.Tensor): A tensor representing the mask for the test set.\n",
    "        train_protein_ids (list): A list of protein IDs for the training set.\n",
    "        val_protein_ids (list): A list of protein IDs for the validation set.\n",
    "        test_protein_ids (list): A list of protein IDs for the test set.\n",
    "        train_go_ids (list): A list of GO IDs for the training set.\n",
    "        val_go_ids (list): A list of GO IDs for the validation set.\n",
    "        test_go_ids (list): A list of GO IDs for the test set.\n",
    "        \"\"\"\n",
    "        self.protein_ids_map = protein_ids_map\n",
    "        self.go_ids_map = go_ids_map\n",
    "        self.ppi_folder = Path(ppi_folder)\n",
    "        self.edge_index_protgo_positive = None\n",
    "        self.edge_index_protgo_negative = None\n",
    "        self.associations_positive_df = None\n",
    "        self.associations_negative_df = None\n",
    "        self.associations_combined_df = None\n",
    "        self.hetero_data = None\n",
    "        self.train_mask = None\n",
    "        self.val_mask = None\n",
    "        self.test_mask = None\n",
    "        self.train_protein_ids = None\n",
    "        self.val_protein_ids = None\n",
    "        self.test_protein_ids = None\n",
    "        self.train_go_ids = None\n",
    "        self.val_go_ids = None\n",
    "        self.test_go_ids = None\n",
    "\n",
    "    def convert_associations_to_edge_index(self, associations_df):\n",
    "        \"\"\"\n",
    "        Convert the associations DataFrame into edge indices.\n",
    "\n",
    "        Parameters:\n",
    "        associations_df (pandas.DataFrame): A DataFrame containing the associations between proteins and GO terms.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: A tensor representing the edge indices for the associations.\n",
    "        \"\"\"\n",
    "        # Map go ids\n",
    "        associations_df[\"go_idx\"] = associations_df[\"go_id\"].map(self.go_ids_map)\n",
    "\n",
    "        # Map proteins ids\n",
    "        associations_df[\"uniprot_idx\"] = associations_df[\"uniprot_id\"].map(\n",
    "            self.protein_ids_map\n",
    "        )\n",
    "\n",
    "        # Save the positive associations\n",
    "        self.associations_positive_df = associations_df\n",
    "\n",
    "        # Create edge index\n",
    "        protein_index = associations_df[\"uniprot_idx\"].values\n",
    "        go_index = associations_df[\"go_idx\"].values\n",
    "        indices = np.stack((protein_index, go_index))\n",
    "        edge_index = torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "        self.edge_index_protgo_positive = edge_index\n",
    "        return edge_index\n",
    "\n",
    "    def generate_negative_edges(\n",
    "        self,\n",
    "        edge_index,\n",
    "        num_nodes_protein,\n",
    "        num_nodes_go,\n",
    "        num_neg_samples,\n",
    "        weighted=False,\n",
    "        random_state=42,\n",
    "        max_attempts_factor=10,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate negative edges for protein-GO associations.\n",
    "\n",
    "        Parameters:\n",
    "        - edge_index (torch.Tensor): The edge index tensor representing existing protein-GO associations.\n",
    "        - num_nodes_protein (int): The number of protein nodes.\n",
    "        - num_nodes_go (int): The number of GO nodes.\n",
    "        - num_neg_samples (int): The number of negative samples to generate.\n",
    "        - weighted (bool, optional): Whether to use weighted sampling for protein nodes. Default is False.\n",
    "        - random_state (int, optional): The random seed for reproducibility. Default is 42.\n",
    "        - max_attempts_factor (int, optional): The maximum number of attempts to generate a valid negative sample. Default is 10.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The tensor representing the generated negative protein-GO associations.\n",
    "        pandas.DataFrame: The DataFrame containing the generated negative protein-GO associations.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        existing_edges = set(map(tuple, edge_index.t().tolist()))\n",
    "\n",
    "        if weighted:\n",
    "            protein_degrees = np.zeros(num_nodes_protein, dtype=int)\n",
    "            for protein_node, go_node in edge_index.t().tolist():\n",
    "                protein_degrees[protein_node] += 1\n",
    "\n",
    "        negative_samples = []\n",
    "        attempts = 0\n",
    "        max_attempts = num_neg_samples * max_attempts_factor\n",
    "        neg_degree_count = np.zeros(num_nodes_protein, dtype=int) if weighted else None\n",
    "\n",
    "        while len(negative_samples) < num_neg_samples and attempts < max_attempts:\n",
    "            if weighted:\n",
    "                protein_node = rng.choice(\n",
    "                    num_nodes_protein, p=protein_degrees / protein_degrees.sum()\n",
    "                )\n",
    "            else:\n",
    "                protein_node = rng.integers(0, num_nodes_protein)\n",
    "\n",
    "            go_node = rng.integers(0, num_nodes_go)\n",
    "\n",
    "            if (protein_node, go_node) not in existing_edges:\n",
    "                negative_samples.append([protein_node, go_node])\n",
    "                if weighted:\n",
    "                    neg_degree_count[protein_node] += 1\n",
    "\n",
    "            attempts += 1\n",
    "\n",
    "        neg_sample_tensor = (\n",
    "            torch.tensor(negative_samples, dtype=torch.long).t()\n",
    "            if negative_samples\n",
    "            else torch.empty((2, 0), dtype=torch.long)\n",
    "        )\n",
    "\n",
    "        protein_mapping = {v: k for k, v in self.protein_ids_map.items()}\n",
    "        go_mapping = {v: k for k, v in self.go_ids_map.items()}\n",
    "\n",
    "        mapped_negative_samples = [\n",
    "            {\n",
    "                \"uniprot_id\": protein_mapping.get(u_idx, None),\n",
    "                \"go_id\": go_mapping.get(g_idx, None),\n",
    "                \"uniprot_idx\": u_idx,\n",
    "                \"go_idx\": g_idx,\n",
    "            }\n",
    "            for u_idx, g_idx in negative_samples\n",
    "        ]\n",
    "\n",
    "        self.associations_negative_df = pd.DataFrame(mapped_negative_samples)\n",
    "        self.edge_index_protgo_negative = neg_sample_tensor\n",
    "\n",
    "        # Ensure no overlap between positive and negative edges\n",
    "        if not self.check_edge_overlap():\n",
    "            raise ValueError(\"Overlap found between positive and negative edges.\")\n",
    "\n",
    "    def combine_associations(self):\n",
    "        \"\"\"\n",
    "        Combine the positive and negative associations DataFrames.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: A DataFrame containing both positive and negative associations.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self.associations_positive_df is None\n",
    "            or self.associations_negative_df is None\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Both positive and negative associations must be generated before combining.\"\n",
    "            )\n",
    "\n",
    "        # Add a label column to differentiate positive and negative associations\n",
    "        self.associations_positive_df[\"label\"] = 1\n",
    "        self.associations_negative_df[\"label\"] = 0\n",
    "\n",
    "        # Combine the DataFrames\n",
    "        self.associations_combined_df = pd.concat(\n",
    "            [self.associations_positive_df, self.associations_negative_df],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        return self.associations_combined_df\n",
    "\n",
    "    def check_balance(\n",
    "        self, positive_edge_index, negative_edge_index, num_nodes_protein, num_nodes_go\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This function checks the degree distribution of proteins and GO terms in the positive and negative edge sets.\n",
    "        It plots the distributions using KDE plots and saves the plot as a PNG file.\n",
    "\n",
    "        Parameters:\n",
    "        - positive_edge_index (torch.Tensor): A tensor representing the edge indices of positive edges.\n",
    "        - negative_edge_index (torch.Tensor): A tensor representing the edge indices of negative edges.\n",
    "        - num_nodes_protein (int): The number of protein nodes in the graph.\n",
    "        - num_nodes_go (int): The number of GO terms in the graph.\n",
    "\n",
    "        Returns:\n",
    "        - None. The function only plots and saves the degree distribution plot.\n",
    "        \"\"\"\n",
    "        protein_degrees_pos = np.zeros(num_nodes_protein, dtype=int)\n",
    "        go_degrees_pos = np.zeros(num_nodes_go, dtype=int)\n",
    "        for protein_node, go_node in positive_edge_index.t().tolist():\n",
    "            protein_degrees_pos[protein_node] += 1\n",
    "            go_degrees_pos[go_node] += 1\n",
    "\n",
    "        protein_degrees_neg = np.zeros(num_nodes_protein, dtype=int)\n",
    "        go_degrees_neg = np.zeros(num_nodes_go, dtype=int)\n",
    "        for protein_node, go_node in negative_edge_index.t().tolist():\n",
    "            protein_degrees_neg[protein_node] += 1\n",
    "            go_degrees_neg[go_node] += 1\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.kdeplot(\n",
    "            protein_degrees_pos, color=\"blue\", label=\"Positive\", fill=True, alpha=0.5\n",
    "        )\n",
    "        sns.kdeplot(\n",
    "            protein_degrees_neg, color=\"red\", label=\"Negative\", fill=True, alpha=0.5\n",
    "        )\n",
    "        plt.title(\"Protein Degree Distribution\")\n",
    "        plt.xlabel(\"Degree\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.kdeplot(\n",
    "            go_degrees_pos, color=\"blue\", label=\"Positive\", fill=True, alpha=0.5\n",
    "        )\n",
    "        sns.kdeplot(go_degrees_neg, color=\"red\", label=\"Negative\", fill=True, alpha=0.5)\n",
    "        plt.title(\"GO Terms Degree Distribution\")\n",
    "        plt.xlabel(\"Degree\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.ppi_folder / \"balance_check.png\", dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    ########################################################################\n",
    "    def create_heterodata(\n",
    "        self,\n",
    "        protein_feats,\n",
    "        go_feats,\n",
    "        protein_ids,\n",
    "        go_ids,\n",
    "        edge_index_ppi,\n",
    "        go_edges_df,\n",
    "        edge_index_protgo_positive,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a HeteroData object by combining protein and GO features,\n",
    "        protein and GO IDs, and protein-protein interaction (PPI) and GO relation edge indices.\n",
    "\n",
    "        Parameters:\n",
    "        protein_feats (torch.Tensor): A tensor containing protein features.\n",
    "        go_feats (torch.Tensor): A tensor containing GO term features.\n",
    "        protein_ids (list): A list of protein IDs.\n",
    "        go_ids (list): A list of GO term IDs.\n",
    "        edge_index_ppi (torch.Tensor): A tensor containing edge indices for PPI.\n",
    "        go_edges_df (pd.DataFrame): DataFrame containing GO term relationships and edge types.\n",
    "        edge_index_protgo_positive (torch.Tensor): A tensor containing positive edge indices for protein-GO associations.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Make PPI graph undirected\n",
    "        edge_index_ppi = to_undirected(edge_index_ppi)\n",
    "\n",
    "        # Concatenate positive and negative edge indices\n",
    "        edge_index_protgo = torch.cat(\n",
    "            (edge_index_protgo_positive, self.edge_index_protgo_negative), dim=1\n",
    "        )\n",
    "\n",
    "        # Create positive and negative labels\n",
    "        positive_labels = torch.ones(\n",
    "            edge_index_protgo_positive.shape[1], dtype=torch.float32\n",
    "        )\n",
    "        negative_labels = torch.zeros(\n",
    "            self.edge_index_protgo_negative.shape[1], dtype=torch.float32\n",
    "        )\n",
    "        edge_labels = torch.cat((positive_labels, negative_labels), dim=0)\n",
    "\n",
    "        # Create a HeteroData object\n",
    "        hetero_data = HeteroData()\n",
    "        hetero_data[\"protein\"].x = protein_feats\n",
    "        hetero_data[\"go\"].x = go_feats\n",
    "        hetero_data[\"protein\"].id = protein_ids\n",
    "        hetero_data[\"go\"].id = go_ids\n",
    "        hetero_data[\"protein\", \"ppi\", \"protein\"].edge_index = edge_index_ppi\n",
    "\n",
    "        # Handle GO edges\n",
    "        go_ids_map = {go_id: idx for idx, go_id in enumerate(go_ids)}\n",
    "        relationship_types = go_edges_df[\"relationship\"].unique()\n",
    "\n",
    "        for relationship in relationship_types:\n",
    "            edges = go_edges_df[go_edges_df[\"relationship\"] == relationship]\n",
    "            source_nodes = [go_ids_map[go_id] for go_id in edges[\"source_go_id\"]]\n",
    "            target_nodes = [go_ids_map[go_id] for go_id in edges[\"target_go_id\"]]\n",
    "            edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "            hetero_data[\"go\", relationship, \"go\"].edge_index = edge_index\n",
    "\n",
    "        # Add protein-GO associations\n",
    "        hetero_data[\"protein\", \"associates_with\", \"go\"].edge_index = edge_index_protgo\n",
    "        hetero_data[\"protein\", \"associates_with\", \"go\"].edge_label = edge_labels\n",
    "\n",
    "        # Assign the HeteroData object to the class attribute\n",
    "        self.hetero_data = hetero_data\n",
    "\n",
    "        # Output information about the GO graph structure\n",
    "        print(\"Heterogeneous Graph Information:\")\n",
    "        print(f\"Number of GO nodes: {hetero_data['go'].x.size(0)}\")\n",
    "        for relationship in relationship_types:\n",
    "            num_edges = hetero_data[\"go\", relationship, \"go\"].edge_index.size(1)\n",
    "            print(f\"GO Relationship: {relationship}, Number of edges: {num_edges}\")\n",
    "\n",
    "    def split_dataset(self, train_ratio=0.6):\n",
    "        \"\"\"\n",
    "        Splits the dataset into training, validation, and testing sets based on the given train ratio.\n",
    "\n",
    "        Parameters:\n",
    "        train_ratio (float, optional): The ratio of the dataset to be used for training. Default is 0.6.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        val_ratio = (1 - train_ratio) / 2\n",
    "        test_ratio = val_ratio\n",
    "        assert isinstance(train_ratio, float), \"Train ratio must be a float.\"\n",
    "        assert 0 < train_ratio < 1, \"Train ratio must be between 0 and 1.\"\n",
    "\n",
    "        unique_proteins, edge_positions = torch.unique(\n",
    "            self.hetero_data[\"protein\", \"associates_with\", \"go\"].edge_index[0],\n",
    "            return_inverse=True,\n",
    "        )\n",
    "        permuted_indices = torch.randperm(unique_proteins.size(0))\n",
    "        num_proteins = unique_proteins.size(0)\n",
    "        train_end = int(num_proteins * train_ratio)\n",
    "        val_end = int(num_proteins * (train_ratio + val_ratio))\n",
    "\n",
    "        train_proteins = permuted_indices[:train_end]\n",
    "        val_proteins = permuted_indices[train_end:val_end]\n",
    "        test_proteins = permuted_indices[val_end:]\n",
    "\n",
    "        train_mask = edge_positions.unsqueeze(0).eq(train_proteins.unsqueeze(1)).any(0)\n",
    "        val_mask = edge_positions.unsqueeze(0).eq(val_proteins.unsqueeze(1)).any(0)\n",
    "        test_mask = edge_positions.unsqueeze(0).eq(test_proteins.unsqueeze(1)).any(0)\n",
    "\n",
    "        self.hetero_data[\"protein\", \"associates_with\", \"go\"][\"train_mask\"] = train_mask\n",
    "        self.hetero_data[\"protein\", \"associates_with\", \"go\"][\"val_mask\"] = val_mask\n",
    "        self.hetero_data[\"protein\", \"associates_with\", \"go\"][\"test_mask\"] = test_mask\n",
    "\n",
    "        self.train_mask = train_mask\n",
    "        self.val_mask = val_mask\n",
    "        self.test_mask = test_mask\n",
    "\n",
    "        protein_mapping = {v: k for k, v in self.protein_ids_map.items()}\n",
    "        go_mapping = {v: k for k, v in self.go_ids_map.items()}\n",
    "\n",
    "        self.train_protein_ids = [protein_mapping[idx.item()] for idx in train_proteins]\n",
    "        self.val_protein_ids = [protein_mapping[idx.item()] for idx in val_proteins]\n",
    "        self.test_protein_ids = [protein_mapping[idx.item()] for idx in test_proteins]\n",
    "\n",
    "        train_go_indices = (\n",
    "            self.hetero_data[\"protein\", \"associates_with\", \"go\"]\n",
    "            .edge_index[1, train_mask]\n",
    "            .tolist()\n",
    "        )\n",
    "        val_go_indices = (\n",
    "            self.hetero_data[\"protein\", \"associates_with\", \"go\"]\n",
    "            .edge_index[1, val_mask]\n",
    "            .tolist()\n",
    "        )\n",
    "        test_go_indices = (\n",
    "            self.hetero_data[\"protein\", \"associates_with\", \"go\"]\n",
    "            .edge_index[1, test_mask]\n",
    "            .tolist()\n",
    "        )\n",
    "\n",
    "        self.train_go_ids = [go_mapping[idx] for idx in train_go_indices]\n",
    "        self.val_go_ids = [go_mapping[idx] for idx in val_go_indices]\n",
    "        self.test_go_ids = [go_mapping[idx] for idx in test_go_indices]\n",
    "\n",
    "    def plot_go_edge_type_ratios(self):\n",
    "        \"\"\"\n",
    "        Plots the ratios of different edge types in the GO graph part of the heterodata.\n",
    "        \"\"\"\n",
    "        go_edge_types = []\n",
    "        edge_counts = []\n",
    "\n",
    "        for edge_type in self.hetero_data.edge_types:\n",
    "            if edge_type[0] == \"go\" and edge_type[2] == \"go\":\n",
    "                go_edge_types.append(edge_type[1])\n",
    "                edge_store = self.hetero_data[edge_type]\n",
    "                edge_counts.append(edge_store.edge_index.size(1))\n",
    "\n",
    "        if not go_edge_types:\n",
    "            print(\"No GO-GO edges found in the heterodata.\")\n",
    "            return\n",
    "\n",
    "        total_edges = sum(edge_counts)\n",
    "        edge_ratios = [count / total_edges for count in edge_counts]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(go_edge_types, edge_ratios)\n",
    "        plt.title(\"GO Edge Type Ratios\")\n",
    "        plt.xlabel(\"Edge Types\")\n",
    "        plt.ylabel(\"Ratio\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "        # Add value labels on top of each bar\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height,\n",
    "                f\"{height:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "\n",
    "        # Add total edge count as text\n",
    "        plt.text(\n",
    "            0.05, 0.95, f\"Total edges: {total_edges}\", transform=plt.gca().transAxes\n",
    "        )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            self.ppi_folder / \"go_edge_type_ratios.png\", dpi=300, bbox_inches=\"tight\"\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        print(\"GO Edge Type Ratios:\")\n",
    "        for edge_type, ratio, count in zip(go_edge_types, edge_ratios, edge_counts):\n",
    "            print(f\"{edge_type}: {ratio:.2f} ({count} edges)\")\n",
    "\n",
    "    def to_device(self, device):\n",
    "        \"\"\"\n",
    "        Moves the HeteroData object and the masks to the specified device.\n",
    "\n",
    "        Parameters:\n",
    "        device (torch.device): The device to move the data to.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.hetero_data = self.hetero_data.to(device)\n",
    "        self.train_mask = self.train_mask.to(device)\n",
    "        self.val_mask = self.val_mask.to(device)\n",
    "        self.test_mask = self.test_mask.to(device)\n",
    "\n",
    "    def create_data_loaders(self, batch_size=32):\n",
    "        \"\"\"\n",
    "        Creates data loaders for training, validation, and testing sets.\n",
    "\n",
    "        Parameters:\n",
    "        batch_size (int, optional): The batch size for the data loaders. Default is 32.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        datasets = {}\n",
    "        masks = {\"train\": self.train_mask, \"val\": self.val_mask, \"test\": self.test_mask}\n",
    "\n",
    "        for split in masks:\n",
    "            edge_index = self.hetero_data[\n",
    "                \"protein\", \"associates_with\", \"go\"\n",
    "            ].edge_index[:, masks[split]]\n",
    "            edge_label = self.hetero_data[\n",
    "                \"protein\", \"associates_with\", \"go\"\n",
    "            ].edge_label[masks[split]]\n",
    "            datasets[split] = TensorDataset(edge_index.t(), edge_label)\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            datasets[\"train\"], batch_size=batch_size, shuffle=True\n",
    "        )\n",
    "        self.val_loader = DataLoader(\n",
    "            datasets[\"val\"], batch_size=batch_size, shuffle=False\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            datasets[\"test\"], batch_size=batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "    def plot_label_ratios(self):\n",
    "        \"\"\"\n",
    "        Plots the label ratios in the training, validation, and testing sets.\n",
    "\n",
    "        Parameters:\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        edge_index = self.hetero_data[\"protein\", \"associates_with\", \"go\"].edge_index\n",
    "        edge_labels = self.hetero_data[\"protein\", \"associates_with\", \"go\"].edge_label\n",
    "\n",
    "        train_labels = edge_labels[self.train_mask]\n",
    "        val_labels = edge_labels[self.val_mask]\n",
    "        test_labels = edge_labels[self.test_mask]\n",
    "\n",
    "        train_label_ratio = torch.sum(train_labels).item() / train_labels.size(0)\n",
    "        val_label_ratio = torch.sum(val_labels).item() / val_labels.size(0)\n",
    "        test_label_ratio = torch.sum(test_labels).item() / test_labels.size(0)\n",
    "\n",
    "        ratios = [train_label_ratio, val_label_ratio, test_label_ratio]\n",
    "        labels = [\"Train\", \"Val\", \"Test\"]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        bars = plt.bar(labels, ratios, color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n",
    "        plt.ylabel(\"Positive Label Ratio\")\n",
    "        plt.title(\"Positive Label Ratios in Train, Validation, and Test Sets\")\n",
    "\n",
    "        # Add value labels on top of each bar\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height,\n",
    "                f\"{height:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "\n",
    "        # Add total counts as text\n",
    "        plt.text(\n",
    "            0.05, 0.95, f\"Train: {train_labels.size(0)}\", transform=plt.gca().transAxes\n",
    "        )\n",
    "        plt.text(\n",
    "            0.05, 0.90, f\"Val: {val_labels.size(0)}\", transform=plt.gca().transAxes\n",
    "        )\n",
    "        plt.text(\n",
    "            0.05, 0.85, f\"Test: {test_labels.size(0)}\", transform=plt.gca().transAxes\n",
    "        )\n",
    "\n",
    "        plt.ylim(0, 1)  # Set y-axis limit from 0 to 1\n",
    "        plt.savefig(self.ppi_folder / \"label_ratios.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()  # Close the plot to free up memory\n",
    "\n",
    "        print(f\"Train positive ratio: {train_label_ratio:.2f}\")\n",
    "        print(f\"Validation positive ratio: {val_label_ratio:.2f}\")\n",
    "        print(f\"Test positive ratio: {test_label_ratio:.2f}\")\n",
    "\n",
    "    def save_heterodata(self):\n",
    "        \"\"\"\n",
    "        Saves the HeteroData object to a specified file path.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        save_file_path = self.ppi_folder / \"heterodata.pt\"\n",
    "        try:\n",
    "            torch.save(self.hetero_data, save_file_path)\n",
    "            print(f\"HeteroData object saved to {save_file_path}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving the HeteroData object: {e}\")\n",
    "\n",
    "    def check_edge_overlap(self):\n",
    "        \"\"\"\n",
    "        Ensure that there is no overlap between positive and negative edges.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if no overlap is found, False otherwise.\n",
    "        \"\"\"\n",
    "        positive_edges_set = set(\n",
    "            map(tuple, self.edge_index_protgo_positive.t().tolist())\n",
    "        )\n",
    "        negative_edges_set = set(\n",
    "            map(tuple, self.edge_index_protgo_negative.t().tolist())\n",
    "        )\n",
    "\n",
    "        # Check for overlap\n",
    "        overlap = positive_edges_set & negative_edges_set\n",
    "        if overlap:\n",
    "            print(f\"Found overlap between positive and negative edges: {overlap}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"No overlap between positive and negative edges.\")\n",
    "            return True\n",
    "\n",
    "    def check_data_overlap(self):\n",
    "        \"\"\"\n",
    "        Ensure that there is no leakage of proteins between train, val, and test sets.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if no leakage is found, False otherwise.\n",
    "        \"\"\"\n",
    "        train_set = set(self.train_protein_ids)\n",
    "        val_set = set(self.val_protein_ids)\n",
    "        test_set = set(self.test_protein_ids)\n",
    "\n",
    "        # Check for overlaps\n",
    "        train_val_overlap = train_set & val_set\n",
    "        train_test_overlap = train_set & test_set\n",
    "        val_test_overlap = val_set & test_set\n",
    "\n",
    "        if train_val_overlap or train_test_overlap or val_test_overlap:\n",
    "            if train_val_overlap:\n",
    "                print(f\"Found overlap between train and val sets: {train_val_overlap}\")\n",
    "            if train_test_overlap:\n",
    "                print(\n",
    "                    f\"Found overlap between train and test sets: {train_test_overlap}\"\n",
    "                )\n",
    "            if val_test_overlap:\n",
    "                print(f\"Found overlap between val and test sets: {val_test_overlap}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"No leakage of proteins between train, val, and test sets.\")\n",
    "            return True\n",
    "\n",
    "    def process(\n",
    "        self,\n",
    "        associations_df,\n",
    "        protein_feats,\n",
    "        go_feats,\n",
    "        protein_ids,\n",
    "        go_ids,\n",
    "        edge_index_ppi,\n",
    "        go_edges_df,\n",
    "        train_ratio=0.6,\n",
    "        ratio=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Processes the dataset by generating negative edges, creating a HeteroData object,\n",
    "        splitting the dataset into training, validation, and testing sets, and plotting label ratios.\n",
    "\n",
    "        Parameters:\n",
    "        associations_df (pandas.DataFrame): DataFrame containing the associations between proteins and GO terms.\n",
    "        protein_feats (torch.Tensor): Protein features.\n",
    "        go_feats (torch.Tensor): GO term features.\n",
    "        protein_ids (list): Protein IDs.\n",
    "        go_ids (list): GO term IDs.\n",
    "        edge_index_ppi (torch.Tensor): Edge indices for protein-protein interactions.\n",
    "        go_edges_df (pandas.DataFrame): DataFrame containing GO term relationships and edge types.\n",
    "        train_ratio (float, optional): The ratio of the dataset to be used for training. Default is 0.6.\n",
    "        ratio (int, optional): The ratio of negative edges to positive edges. Default is 1.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        edge_index_protgo_positive = self.convert_associations_to_edge_index(\n",
    "            associations_df\n",
    "        )\n",
    "        number_pos_edges = edge_index_protgo_positive.shape[1]\n",
    "\n",
    "        if self.structured_negative_sampling_feasible(edge_index_protgo_positive):\n",
    "            print(f\"Generating {number_pos_edges * ratio} negative edges...\")\n",
    "\n",
    "            self.generate_negative_edges(\n",
    "                edge_index=edge_index_protgo_positive,\n",
    "                num_nodes_protein=len(protein_ids),\n",
    "                num_nodes_go=len(go_ids),\n",
    "                num_neg_samples=number_pos_edges * ratio,\n",
    "                weighted=True,\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Protein negative associates with GO edge index shape: {self.edge_index_protgo_negative.shape}\"\n",
    "            )\n",
    "            self.check_balance(\n",
    "                edge_index_protgo_positive,\n",
    "                self.edge_index_protgo_negative,\n",
    "                len(protein_ids),\n",
    "                len(go_ids),\n",
    "            )\n",
    "\n",
    "            self.combine_associations()\n",
    "\n",
    "            self.create_heterodata(\n",
    "                protein_feats,\n",
    "                go_feats,\n",
    "                protein_ids,\n",
    "                go_ids,\n",
    "                edge_index_ppi,\n",
    "                go_edges_df,\n",
    "                edge_index_protgo_positive,\n",
    "            )\n",
    "\n",
    "            self.split_dataset(train_ratio)\n",
    "\n",
    "            if not self.check_data_overlap():\n",
    "                raise ValueError(\n",
    "                    \"Protein leakage detected between train, val, and test sets.\"\n",
    "                )\n",
    "\n",
    "            self.plot_label_ratios()\n",
    "            self.plot_go_edge_type_ratios()\n",
    "            self.save_heterodata()\n",
    "\n",
    "        else:\n",
    "            print(\"Negative edges generation isn't feasible.\")\n",
    "\n",
    "    def structured_negative_sampling_feasible(self, edge_index_protgo_positive):\n",
    "        \"\"\"\n",
    "        Checks if structured negative sampling is feasible for the given positive edge indices.\n",
    "\n",
    "        Parameters:\n",
    "        edge_index_protgo_positive (torch.Tensor): Edge indices for positive protein-GO associations.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if structured negative sampling is feasible, False otherwise.\n",
    "        \"\"\"\n",
    "        return structured_negative_sampling_feasible(edge_index_protgo_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAN Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNNGAT(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        protein_features,\n",
    "        go_features,\n",
    "        out_features,\n",
    "        dropout=0.6,\n",
    "        heads=8,\n",
    "        concat=True,\n",
    "    ):\n",
    "        super(HGNNGAT, self).__init__()\n",
    "        per_head_features = out_features // heads if concat else out_features\n",
    "\n",
    "        self.conv1_protein = GATConv(\n",
    "            protein_features,\n",
    "            per_head_features,\n",
    "            heads=heads,\n",
    "            concat=concat,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.conv1_go = GATConv(\n",
    "            go_features, per_head_features, heads=heads, concat=concat, dropout=dropout\n",
    "        )\n",
    "        self.conv2_protein = GATConv(\n",
    "            out_features, per_head_features, heads=heads, concat=concat, dropout=dropout\n",
    "        )\n",
    "        self.conv2_go = GATConv(\n",
    "            out_features, per_head_features, heads=heads, concat=concat, dropout=dropout\n",
    "        )\n",
    "\n",
    "        final_out_features = out_features if concat else per_head_features\n",
    "        self.protein_to_go = Linear(final_out_features * 2, 1)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, mask=None):\n",
    "        x_protein = F.elu(\n",
    "            self.conv1_protein(\n",
    "                x_dict[\"protein\"], edge_index_dict[(\"protein\", \"ppi\", \"protein\")]\n",
    "            )\n",
    "        )\n",
    "        x_go = F.elu(\n",
    "            self.conv1_go(x_dict[\"go\"], edge_index_dict[(\"go\", \"relation\", \"go\")])\n",
    "        )\n",
    "\n",
    "        x_protein = F.elu(\n",
    "            self.conv2_protein(\n",
    "                x_protein, edge_index_dict[(\"protein\", \"ppi\", \"protein\")]\n",
    "            )\n",
    "        )\n",
    "        x_go = F.elu(self.conv2_go(x_go, edge_index_dict[(\"go\", \"relation\", \"go\")]))\n",
    "\n",
    "        edge_index_protein_go = edge_index_dict[(\"protein\", \"associates_with\", \"go\")]\n",
    "        if mask is not None:\n",
    "            edge_index_protein_go = edge_index_protein_go[:, mask]\n",
    "        protein_features = x_protein[edge_index_protein_go[0]]\n",
    "        go_features = x_go[edge_index_protein_go[1]]\n",
    "\n",
    "        association_features = torch.cat([protein_features, go_features], dim=1)\n",
    "        association_scores = self.protein_to_go(association_features).squeeze()\n",
    "        return association_scores\n",
    "\n",
    "    def get_attention_weights(self, x_dict, edge_index_dict):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            def safe_unpack(result):\n",
    "                if isinstance(result, tuple) and len(result) == 2:\n",
    "                    return result\n",
    "                else:\n",
    "                    return result, None\n",
    "\n",
    "            x_protein, attention_protein_1 = safe_unpack(\n",
    "                self.conv1_protein(\n",
    "                    x_dict[\"protein\"],\n",
    "                    edge_index_dict[(\"protein\", \"ppi\", \"protein\")],\n",
    "                    return_attention_weights=True,\n",
    "                )\n",
    "            )\n",
    "            x_protein, attention_protein_2 = safe_unpack(\n",
    "                self.conv2_protein(\n",
    "                    x_protein,\n",
    "                    edge_index_dict[(\"protein\", \"ppi\", \"protein\")],\n",
    "                    return_attention_weights=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            x_go, attention_go_1 = safe_unpack(\n",
    "                self.conv1_go(\n",
    "                    x_dict[\"go\"],\n",
    "                    edge_index_dict[(\"go\", \"relation\", \"go\")],\n",
    "                    return_attention_weights=True,\n",
    "                )\n",
    "            )\n",
    "            x_go, attention_go_2 = safe_unpack(\n",
    "                self.conv2_go(\n",
    "                    x_go,\n",
    "                    edge_index_dict[(\"go\", \"relation\", \"go\")],\n",
    "                    return_attention_weights=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            return (\n",
    "                attention_protein_1,\n",
    "                attention_go_1,\n",
    "                attention_protein_2,\n",
    "                attention_go_2,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FocusedGATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom GAT layer definition\n",
    "\n",
    "class FocusedGATConv(GATConv):\n",
    "    def __init__(\n",
    "        self, *args, sample_size=8, high_degree_threshold=10, directed=False, **kwargs\n",
    "    ):\n",
    "        super(FocusedGATConv, self).__init__(*args, **kwargs)\n",
    "        self.sample_size = sample_size\n",
    "        self.high_degree_threshold = high_degree_threshold\n",
    "        self.directed = directed\n",
    "\n",
    "    def forward(self, x, edge_index, return_attention_weights=False):\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, x.size(0), dtype=x.dtype)\n",
    "\n",
    "        # Create mask for high-degree nodes\n",
    "        high_deg_mask = deg > self.high_degree_threshold\n",
    "\n",
    "        # Keep all edges for low-degree nodes\n",
    "        low_deg_edges = edge_index[:, ~high_deg_mask[row]]\n",
    "\n",
    "        # Sample edges for high-degree nodes\n",
    "        high_deg_edges = []\n",
    "        high_deg_nodes = high_deg_mask.nonzero(as_tuple=True)[0]\n",
    "        for node in high_deg_nodes:\n",
    "            if self.directed:\n",
    "                # For directed graphs, consider only outgoing edges\n",
    "                neighbors = col[row == node]\n",
    "            else:\n",
    "                # For undirected graphs, consider all connected nodes\n",
    "                neighbors = torch.cat([col[row == node], row[col == node]])\n",
    "\n",
    "            if len(neighbors) > self.sample_size:\n",
    "                sampled_neighbors = neighbors[\n",
    "                    torch.randperm(len(neighbors))[: self.sample_size]\n",
    "                ]\n",
    "            else:\n",
    "                sampled_neighbors = neighbors\n",
    "\n",
    "            if self.directed:\n",
    "                sampled_edges = torch.stack(\n",
    "                    [torch.full_like(sampled_neighbors, node), sampled_neighbors], dim=0\n",
    "                )\n",
    "            else:\n",
    "                sampled_edges = torch.stack(\n",
    "                    [\n",
    "                        torch.cat(\n",
    "                            [\n",
    "                                torch.full_like(sampled_neighbors, node),\n",
    "                                sampled_neighbors,\n",
    "                            ]\n",
    "                        ),\n",
    "                        torch.cat(\n",
    "                            [\n",
    "                                sampled_neighbors,\n",
    "                                torch.full_like(sampled_neighbors, node),\n",
    "                            ]\n",
    "                        ),\n",
    "                    ],\n",
    "                    dim=0,\n",
    "                )\n",
    "\n",
    "            high_deg_edges.append(sampled_edges)\n",
    "\n",
    "        if high_deg_edges:\n",
    "            high_deg_edges = torch.cat(high_deg_edges, dim=1)\n",
    "            sampled_edges = torch.cat([low_deg_edges, high_deg_edges], dim=1)\n",
    "        else:\n",
    "            sampled_edges = low_deg_edges\n",
    "\n",
    "        # Call the original forward method to compute attention\n",
    "        out = super().forward(x, sampled_edges, return_attention_weights=True)\n",
    "        x, (sampled_edges, alpha) = out\n",
    "\n",
    "        # Apply softmax normalization to the attention coefficients\n",
    "        alpha = F.softmax(alpha, dim=1)\n",
    "\n",
    "        if return_attention_weights:\n",
    "            return x, (sampled_edges, alpha)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class FocusedHGNNGATdirect(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        protein_features,\n",
    "        go_features,\n",
    "        out_features,\n",
    "        dropout=0.6,\n",
    "        heads=8,\n",
    "        concat=True,\n",
    "        protein_sample_size=8,\n",
    "        protein_high_degree_threshold=10,\n",
    "        go_sample_size=8,\n",
    "        go_high_degree_threshold=20,\n",
    "    ):\n",
    "        super(FocusedHGNNGATdirect, self).__init__()\n",
    "        per_head_features = out_features // heads if concat else out_features\n",
    "\n",
    "        self.conv1_protein = FocusedGATConv(\n",
    "            protein_features,\n",
    "            per_head_features,\n",
    "            heads=heads,\n",
    "            concat=concat,\n",
    "            dropout=dropout,\n",
    "            sample_size=protein_sample_size,\n",
    "            high_degree_threshold=protein_high_degree_threshold,\n",
    "            directed=False,\n",
    "        )\n",
    "        self.conv1_go = FocusedGATConv(\n",
    "            go_features,\n",
    "            per_head_features,\n",
    "            heads=heads,\n",
    "            concat=concat,\n",
    "            dropout=dropout,\n",
    "            sample_size=go_sample_size,\n",
    "            high_degree_threshold=go_high_degree_threshold,\n",
    "            directed=True,\n",
    "        )\n",
    "        self.conv2_protein = FocusedGATConv(\n",
    "            out_features,\n",
    "            per_head_features,\n",
    "            heads=heads,\n",
    "            concat=concat,\n",
    "            dropout=dropout,\n",
    "            sample_size=protein_sample_size,\n",
    "            high_degree_threshold=protein_high_degree_threshold,\n",
    "            directed=False,\n",
    "        )\n",
    "        self.conv2_go = FocusedGATConv(\n",
    "            out_features,\n",
    "            per_head_features,\n",
    "            heads=heads,\n",
    "            concat=concat,\n",
    "            dropout=dropout,\n",
    "            sample_size=go_sample_size,\n",
    "            high_degree_threshold=go_high_degree_threshold,\n",
    "            directed=True,\n",
    "        )\n",
    "\n",
    "        final_out_features = out_features if concat else per_head_features\n",
    "        self.protein_to_go = Linear(final_out_features * 2, 1)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, mask=None):\n",
    "        x_protein = F.elu(\n",
    "            self.conv1_protein(\n",
    "                x_dict[\"protein\"], edge_index_dict[(\"protein\", \"ppi\", \"protein\")]\n",
    "            )\n",
    "        )\n",
    "        x_go = F.elu(\n",
    "            self.conv1_go(x_dict[\"go\"], edge_index_dict[(\"go\", \"relation\", \"go\")])\n",
    "        )\n",
    "\n",
    "        x_protein = F.elu(\n",
    "            self.conv2_protein(\n",
    "                x_protein, edge_index_dict[(\"protein\", \"ppi\", \"protein\")]\n",
    "            )\n",
    "        )\n",
    "        x_go = F.elu(self.conv2_go(x_go, edge_index_dict[(\"go\", \"relation\", \"go\")]))\n",
    "\n",
    "        edge_index_protein_go = edge_index_dict[(\"protein\", \"associates_with\", \"go\")]\n",
    "        if mask is not None:\n",
    "            edge_index_protein_go = edge_index_protein_go[:, mask]\n",
    "        protein_features = x_protein[edge_index_protein_go[0]]\n",
    "        go_features = x_go[edge_index_protein_go[1]]\n",
    "\n",
    "        association_features = torch.cat([protein_features, go_features], dim=1)\n",
    "        association_scores = self.protein_to_go(association_features).squeeze()\n",
    "        return association_scores\n",
    "\n",
    "    def get_attention_weights(self, x_dict, edge_index_dict):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            _, attention_protein_1 = self.conv1_protein(\n",
    "                x_dict[\"protein\"],\n",
    "                edge_index_dict[(\"protein\", \"ppi\", \"protein\")],\n",
    "                return_attention_weights=True,\n",
    "            )\n",
    "            _, attention_protein_2 = self.conv2_protein(\n",
    "                F.elu(\n",
    "                    self.conv1_protein(\n",
    "                        x_dict[\"protein\"],\n",
    "                        edge_index_dict[(\"protein\", \"ppi\", \"protein\")],\n",
    "                    )\n",
    "                ),\n",
    "                edge_index_dict[(\"protein\", \"ppi\", \"protein\")],\n",
    "                return_attention_weights=True,\n",
    "            )\n",
    "\n",
    "            _, attention_go_1 = self.conv1_go(\n",
    "                x_dict[\"go\"],\n",
    "                edge_index_dict[(\"go\", \"relation\", \"go\")],\n",
    "                return_attention_weights=True,\n",
    "            )\n",
    "            _, attention_go_2 = self.conv2_go(\n",
    "                F.elu(\n",
    "                    self.conv1_go(\n",
    "                        x_dict[\"go\"], edge_index_dict[(\"go\", \"relation\", \"go\")]\n",
    "                    )\n",
    "                ),\n",
    "                edge_index_dict[(\"go\", \"relation\", \"go\")],\n",
    "                return_attention_weights=True,\n",
    "            )\n",
    "\n",
    "            return [\n",
    "                attention_protein_1,\n",
    "                attention_go_1,\n",
    "                attention_protein_2,\n",
    "                attention_go_2,\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_dict, edge_index_dict, edge_labels, val_mask, device=\"cuda\"):\n",
    "    \"\"\"Evaluate train and validation during training.\"\"\"\n",
    "    model.to(device)\n",
    "    for key in x_dict:\n",
    "        x_dict[key] = x_dict[key].to(device)\n",
    "    for key in edge_index_dict:\n",
    "        edge_index_dict[key] = edge_index_dict[key].to(device)\n",
    "    edge_labels = edge_labels.to(device)\n",
    "    val_mask = val_mask.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x_dict, edge_index_dict, val_mask)\n",
    "        val_loss = F.binary_cross_entropy_with_logits(out, edge_labels[val_mask])\n",
    "        predictions = out.sigmoid()\n",
    "        val_auroc = roc_auc_score(\n",
    "            edge_labels[val_mask].cpu().numpy(),\n",
    "            predictions.cpu().numpy(),\n",
    "        )\n",
    "    return val_loss.item(), val_auroc\n",
    "\n",
    "\n",
    "def predict(model, x_dict, edge_index_dict, edge_labels, mask, device=\"cuda\"):\n",
    "    \"\"\"Make predictions (test)\"\"\"\n",
    "    model.to(device)\n",
    "    for key in x_dict:\n",
    "        x_dict[key] = x_dict[key].to(device)\n",
    "    for key in edge_index_dict:\n",
    "        edge_index_dict[key] = edge_index_dict[key].to(device)\n",
    "    edge_labels = edge_labels.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x_dict, edge_index_dict, mask)\n",
    "        probabilities = out.sigmoid()\n",
    "\n",
    "        true_labels = edge_labels[mask]\n",
    "\n",
    "        auroc = roc_auc_score(true_labels.cpu().numpy(), probabilities.cpu().numpy())\n",
    "\n",
    "        predicted_labels = (probabilities > 0.5).float()\n",
    "\n",
    "        return predicted_labels, true_labels, auroc, probabilities\n",
    "\n",
    "\n",
    "def evaluate_model_performance(\n",
    "    model,\n",
    "    hetero_data,\n",
    "    associations_df,\n",
    "    go_id_map,\n",
    "    save_path,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a trained model on a test set.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model to evaluate.\n",
    "    - hetero_data: The heterogeneous graph data containing the protein, GO, and associations.\n",
    "    - associations_df: The DataFrame containing the associations between proteins and GO terms.\n",
    "    - go_id_map: The mapping of GO term indices to their IDs.\n",
    "    - save_path: The path to save the test results and plots.\n",
    "    - device: The device to run the model and data on (default is 'cuda' if available, otherwise 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - results_df: The DataFrame containing the test results, including protein IDs, GO IDs, ground truth labels, predicted labels, and probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure Path object of save path\n",
    "    save_path = Path(save_path)\n",
    "\n",
    "    # Move model and data to device\n",
    "    model.to(device)\n",
    "    for key in hetero_data.x_dict:\n",
    "        hetero_data.x_dict[key] = hetero_data.x_dict[key].to(device)\n",
    "    for key in hetero_data.edge_index_dict:\n",
    "        hetero_data.edge_index_dict[key] = hetero_data.edge_index_dict[key].to(device)\n",
    "    hetero_data[\"protein\", \"associates_with\", \"go\"].edge_label = hetero_data[\n",
    "        \"protein\", \"associates_with\", \"go\"\n",
    "    ].edge_label.to(device)\n",
    "    hetero_data[\"protein\", \"associates_with\", \"go\"].test_mask = hetero_data[\n",
    "        \"protein\", \"associates_with\", \"go\"\n",
    "    ].test_mask.to(device)\n",
    "\n",
    "    # Extract test mask\n",
    "    test_mask = hetero_data[\"protein\", \"associates_with\", \"go\"].test_mask\n",
    "\n",
    "    # Predict\n",
    "    predicted_labels, test_labels, test_auroc, probabilities = predict(\n",
    "        model,\n",
    "        hetero_data.x_dict,\n",
    "        hetero_data.edge_index_dict,\n",
    "        hetero_data[\"protein\", \"associates_with\", \"go\"].edge_label,\n",
    "        test_mask,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Retrieve the indices of the test edges\n",
    "    test_edge_indices = hetero_data[\"protein\", \"associates_with\", \"go\"].edge_index[\n",
    "        :, test_mask\n",
    "    ]\n",
    "\n",
    "    # Get the protein and GO indices\n",
    "    protein_indices = test_edge_indices[0].cpu().numpy()\n",
    "    go_indices = test_edge_indices[1].cpu().numpy()\n",
    "\n",
    "    # Map indices to IDs\n",
    "    protein_id_map = dict(\n",
    "        zip(\n",
    "            associations_df[\"uniprot_idx\"].astype(int),\n",
    "            associations_df[\"uniprot_id\"],\n",
    "        )\n",
    "    )\n",
    "    go_id_map = {v: k for k, v in go_id_map.items()}  # swap mapping\n",
    "\n",
    "    # Prepare data for the DataFrame\n",
    "    data = {\n",
    "        \"Protein ID\": [protein_id_map.get(idx, \"Unknown\") for idx in protein_indices],\n",
    "        \"GO ID\": [go_id_map.get(idx, \"Unknown\") for idx in go_indices],\n",
    "        \"Ground Truth Label\": [label.item() for label in test_labels],\n",
    "        \"Predicted Label\": [label.item() for label in predicted_labels],\n",
    "        \"Probability\": [prob.item() for prob in probabilities],\n",
    "    }\n",
    "\n",
    "    # Create and save df\n",
    "    results_df = pd.DataFrame(data)\n",
    "    results_df.to_csv(save_path / \"test_results.csv\", index=False)\n",
    "\n",
    "    # Plot metrics\n",
    "    print(f\"AUROC test: {test_auroc:.2f}\")\n",
    "\n",
    "    # Save classification report and print\n",
    "    class_report = classification_report(\n",
    "        test_labels.cpu().numpy(),\n",
    "        predicted_labels.cpu().numpy(),\n",
    "        target_names=[\"0\", \"1\"],\n",
    "    )\n",
    "\n",
    "    with open(save_path / \"classification_report.txt\", \"w\") as file:\n",
    "        file.write(class_report)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        test_labels.cpu().numpy(),\n",
    "        predicted_labels.cpu().numpy(),\n",
    "        normalize=\"true\",\n",
    "        cmap=\"Blues\",\n",
    "        values_format=\".1%\",\n",
    "    )\n",
    "    plt.savefig(save_path / \"confusion_matrix.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    dataset,\n",
    "    config,\n",
    "    run,\n",
    "    save_path=None,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    use_scheduler=True,\n",
    "    accumulation_steps=1,\n",
    "):\n",
    "\n",
    "    model.to(device)\n",
    "    dataset.to_device(device)\n",
    "    dataset.create_data_loaders(batch_size=config[\"batch_size\"])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    scheduler = (\n",
    "        torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            \"min\",\n",
    "            patience=config[\"scheduler_patience\"],\n",
    "            factor=0.1,\n",
    "            verbose=True,\n",
    "        )\n",
    "        if use_scheduler\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in tqdm(\n",
    "        range(1, config[\"epochs\"] + 1), desc=\"Training...\", total=config[\"epochs\"]\n",
    "    ):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_predictions, train_labels = [], []\n",
    "\n",
    "        for i, (edge_index, edge_label) in enumerate(dataset.train_loader):\n",
    "            edge_index, edge_label = edge_index.to(device), edge_label.to(device)\n",
    "            out = model(\n",
    "                {\n",
    "                    \"protein\": dataset.hetero_data[\"protein\"].x,\n",
    "                    \"go\": dataset.hetero_data[\"go\"].x,\n",
    "                },\n",
    "                {\n",
    "                    (\"protein\", \"ppi\", \"protein\"): dataset.hetero_data[\n",
    "                        \"protein\", \"ppi\", \"protein\"\n",
    "                    ].edge_index,\n",
    "                    (\"go\", \"relation\", \"go\"): dataset.hetero_data[\n",
    "                        \"go\", \"relation\", \"go\"\n",
    "                    ].edge_index,\n",
    "                    (\"protein\", \"associates_with\", \"go\"): edge_index.t(),\n",
    "                },\n",
    "            )\n",
    "            loss = (\n",
    "                F.binary_cross_entropy_with_logits(out, edge_label) / accumulation_steps\n",
    "            )\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item() * accumulation_steps\n",
    "            train_predictions.append(out.sigmoid().detach().cpu().numpy())\n",
    "            train_labels.append(edge_label.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(dataset.train_loader)\n",
    "        train_auroc = roc_auc_score(\n",
    "            np.concatenate(train_labels), np.concatenate(train_predictions)\n",
    "        )\n",
    "        wandb.log({\"train_loss\": train_loss, \"train_auroc\": train_auroc})\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            val_predictions, val_labels = [], []\n",
    "\n",
    "            for edge_index, edge_label in dataset.val_loader:\n",
    "                edge_index, edge_label = edge_index.to(device), edge_label.to(device)\n",
    "                out = model(\n",
    "                    {\n",
    "                        \"protein\": dataset.hetero_data[\"protein\"].x,\n",
    "                        \"go\": dataset.hetero_data[\"go\"].x,\n",
    "                    },\n",
    "                    {\n",
    "                        (\"protein\", \"ppi\", \"protein\"): dataset.hetero_data[\n",
    "                            \"protein\", \"ppi\", \"protein\"\n",
    "                        ].edge_index,\n",
    "                        (\"go\", \"relation\", \"go\"): dataset.hetero_data[\n",
    "                            \"go\", \"relation\", \"go\"\n",
    "                        ].edge_index,\n",
    "                        (\"protein\", \"associates_with\", \"go\"): edge_index.t(),\n",
    "                    },\n",
    "                )\n",
    "                loss = F.binary_cross_entropy_with_logits(out, edge_label)\n",
    "                val_loss += loss.item()\n",
    "                val_predictions.append(out.sigmoid().cpu().numpy())\n",
    "                val_labels.append(edge_label.cpu().numpy())\n",
    "\n",
    "            val_loss /= len(dataset.val_loader)\n",
    "            val_auroc = roc_auc_score(\n",
    "                np.concatenate(val_labels), np.concatenate(val_predictions)\n",
    "            )\n",
    "            wandb.log({\"val_loss\": val_loss, \"val_auroc\": val_auroc})\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f\"{save_path}/best_model_weights.pt\")\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "\n",
    "            if early_stopping_counter >= config[\"early_stopping_patience\"]:\n",
    "                print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                break\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch}: Train Loss: {train_loss:.3f}, Train AUROC: {train_auroc:.3f}, \"\n",
    "                f\"Val Loss: {val_loss:.3f}, Val AUROC: {val_auroc:.3f}\"\n",
    "            )\n",
    "    return model, best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainability Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explainability:\n",
    "    def __init__(self, model, hetero_data, save_path):\n",
    "        \"\"\"\n",
    "        Initialize the ExplainabilityModule with the given model, heterogeneous data, and save path.\n",
    "\n",
    "        Parameters:\n",
    "        - model (torch.nn.Module): The graph neural network model to be explained.\n",
    "        - hetero_data (torch_geometric.data.HeteroData): The heterogeneous graph data containing nodes and edges.\n",
    "        - save_path (str): The path where the generated visualizations will be saved.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.hetero_data = hetero_data\n",
    "        self.save_path = Path(save_path)\n",
    "        self.save_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.id_mappings = {\n",
    "            \"protein\": {idx: id for idx, id in enumerate(hetero_data[\"protein\"].id)},\n",
    "            \"go\": {idx: id for idx, id in enumerate(hetero_data[\"go\"].id)},\n",
    "        }\n",
    "\n",
    "    def aggregate_attention_weights(self, attn_weights, edge_index, num_nodes):\n",
    "        node_attention = torch.zeros(\n",
    "            (num_nodes, attn_weights.size(1)), device=attn_weights.device\n",
    "        )\n",
    "        src, dst = edge_index\n",
    "        node_attention.index_add_(0, src, attn_weights)\n",
    "        node_attention.index_add_(0, dst, attn_weights)\n",
    "        return node_attention / 2\n",
    "\n",
    "    def visualize_node_importance(\n",
    "        self, attn_weights, node_type, id_mapping, conv_layer, top_n=10\n",
    "    ):\n",
    "        if attn_weights is None or not attn_weights.numel():\n",
    "            print(\"Attention weights are empty or invalid.\")\n",
    "            return [], None\n",
    "\n",
    "        attn_weights = attn_weights.mean(dim=1).cpu().detach().numpy()\n",
    "        valid_weights = np.where(np.isfinite(attn_weights), attn_weights, 0)\n",
    "\n",
    "        if not np.any(valid_weights):\n",
    "            print(\"All attention weights are zero or invalid.\")\n",
    "            return [], None\n",
    "\n",
    "        top_indices = np.argsort(valid_weights)[-top_n:][::-1]\n",
    "        top_ids = [id_mapping[idx] for idx in top_indices]\n",
    "\n",
    "        return top_ids, valid_weights[top_indices]\n",
    "\n",
    "    def plot_node_importance(self, top_n=25):\n",
    "        attention_weights = self.model.get_attention_weights(\n",
    "            self.hetero_data.x_dict, self.hetero_data.edge_index_dict\n",
    "        )\n",
    "        results = {}\n",
    "        plots = []\n",
    "\n",
    "        num_layers = len(attention_weights) // 2\n",
    "        fig = make_subplots(\n",
    "            rows=2,\n",
    "            cols=num_layers,\n",
    "            subplot_titles=[\n",
    "                f\"{node_type.capitalize()} Layer {i+1}\"\n",
    "                for node_type in [\"Protein\", \"GO\"]\n",
    "                for i in range(num_layers)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        for node_type, attn_weights_list, edge_indices_list in [\n",
    "            (\n",
    "                \"protein\",\n",
    "                [aw[1] for aw in attention_weights[::2]],\n",
    "                [aw[0] for aw in attention_weights[::2]],\n",
    "            ),\n",
    "            (\n",
    "                \"go\",\n",
    "                [aw[1] for aw in attention_weights[1::2]],\n",
    "                [aw[0] for aw in attention_weights[1::2]],\n",
    "            ),\n",
    "        ]:\n",
    "            for i, (attn, edges) in enumerate(\n",
    "                zip(attn_weights_list, edge_indices_list), 1\n",
    "            ):\n",
    "                attention = self.aggregate_attention_weights(\n",
    "                    attn, edges, self.hetero_data[node_type].x.size(0)\n",
    "                )\n",
    "                top_ids, top_weights = self.visualize_node_importance(\n",
    "                    attention,\n",
    "                    node_type.capitalize(),\n",
    "                    self.id_mappings[node_type],\n",
    "                    conv_layer=i,\n",
    "                    top_n=top_n,\n",
    "                )\n",
    "\n",
    "                if top_ids and top_weights is not None:\n",
    "                    row = 1 if node_type == \"protein\" else 2\n",
    "                    fig.add_trace(go.Bar(x=top_ids, y=top_weights), row=row, col=i)\n",
    "                    fig.update_xaxes(\n",
    "                        title_text=f\"Top {top_n} {node_type.capitalize()} Node IDs\",\n",
    "                        row=row,\n",
    "                        col=i,\n",
    "                    )\n",
    "                    fig.update_yaxes(title_text=\"Importance\", row=row, col=i)\n",
    "\n",
    "                results[f\"{node_type}_{i}\"] = top_ids\n",
    "\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            width=300 * num_layers,\n",
    "            title_text=\"Node Importance Across Layers\",\n",
    "        )\n",
    "        plots.append(fig)\n",
    "\n",
    "        fig.write_image(str(self.save_path / \"node_importance_all_layers.png\"))\n",
    "\n",
    "        return results, plots\n",
    "\n",
    "    def compare_degrees(self, important_nodes, node_type, edge_index):\n",
    "        index_mapping = {v: k for k, v in self.id_mappings[node_type].items()}\n",
    "        important_indices = [index_mapping[node_id] for node_id in important_nodes]\n",
    "        row, col = edge_index\n",
    "        node_degrees = degree(row, row.max().item() + 1).cpu().numpy()\n",
    "        important_degrees = [node_degrees[idx] for idx in important_indices]\n",
    "\n",
    "        comparison_df = pd.DataFrame(\n",
    "            {\"Node ID\": important_nodes, \"Degree\": important_degrees}\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.bar(comparison_df[\"Node ID\"], comparison_df[\"Degree\"])\n",
    "        ax.set_xlabel(f\"{node_type} Node IDs\")\n",
    "        ax.set_ylabel(\"Degree\")\n",
    "        ax.set_title(f\"Degree Comparison of Important {node_type} Nodes\")\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.save_path / f\"degree_comparison_attention_{node_type}.png\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        return comparison_df, fig\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_tfidf(df, text_col, label, top_n=25):\n",
    "        vectorizer = TfidfVectorizer(max_df=0.85, min_df=2, ngram_range=(1, 2))\n",
    "        tf_idf_matrix = vectorizer.fit_transform(df[text_col])\n",
    "        max_scores = np.max(tf_idf_matrix, axis=0).toarray().flatten()\n",
    "        sorted_indices = np.argsort(max_scores)[::-1][:top_n]\n",
    "        sorted_features = vectorizer.get_feature_names_out()[sorted_indices]\n",
    "        sorted_scores = max_scores[sorted_indices]\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(sorted_features, sorted_scores, color=\"green\")\n",
    "        plt.xlabel(\"Max TF-IDF Score\")\n",
    "        plt.title(f\"Top {top_n} Terms with Highest TF-IDF Scores in {label}\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.close()\n",
    "\n",
    "        return sorted_features.tolist()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_kde_with_overlay(df, top_go_df, information_accretion_col, label):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.kdeplot(\n",
    "            df[information_accretion_col], fill=True, alpha=0.5, label=\"All GO Terms\"\n",
    "        )\n",
    "        sns.rugplot(\n",
    "            top_go_df[information_accretion_col],\n",
    "            height=0.1,\n",
    "            color=\"red\",\n",
    "            label=\"Top GO Terms\",\n",
    "        )\n",
    "        plt.title(\n",
    "            f\"Distribution of 'ia' across all GO terms with overlay of top GO terms - {label}\"\n",
    "        )\n",
    "        plt.xlabel(\"ia\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_heatmap(\n",
    "        df,\n",
    "        top_go_ids_conv1,\n",
    "        top_go_ids_conv2,\n",
    "        information_accretion_col,\n",
    "        top_n=25,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        top_go_df_conv1 = df[df[\"go_id\"].isin(top_go_ids_conv1)]\n",
    "        top_go_df_conv2 = df[df[\"go_id\"].isin(top_go_ids_conv2)]\n",
    "        num_top_terms = len(top_go_df_conv1)\n",
    "\n",
    "        random_go_df = df[\n",
    "            ~df[\"go_id\"].isin(top_go_ids_conv1 + top_go_ids_conv2)\n",
    "        ].sample(n=num_top_terms, random_state=random_state)\n",
    "        combined_df = pd.concat([random_go_df, top_go_df_conv1, top_go_df_conv2])\n",
    "        combined_df[\"Category\"] = (\n",
    "            [\"Random GO Terms\"] * num_top_terms\n",
    "            + [\"Top GO Terms Conv1\"] * num_top_terms\n",
    "            + [\"Top GO Terms Conv2\"] * num_top_terms\n",
    "        )\n",
    "\n",
    "        plt.figure(figsize=(12, 0.8 * top_n))\n",
    "        ia_matrix = combined_df.pivot_table(\n",
    "            index=\"go_id\", columns=\"Category\", values=information_accretion_col\n",
    "        )\n",
    "        sns.heatmap(\n",
    "            ia_matrix, annot=True, cmap=\"viridis\", cbar_kws={\"label\": \"IA Value\"}\n",
    "        )\n",
    "        plt.title(\n",
    "            \"Heatmap of IA Values for Random GO Terms, Conv1, and Conv2 Top GO Terms\"\n",
    "        )\n",
    "        plt.xlabel(\"Category\")\n",
    "        plt.ylabel(\"GO Term ID\")\n",
    "        plt.close()\n",
    "\n",
    "    def analyze_top_go(\n",
    "        self,\n",
    "        df,\n",
    "        results,\n",
    "        information_accretion_col=\"ia\",\n",
    "        text_col=\"definition\",\n",
    "        top_n=25,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        top_go_df_conv1 = df[df[\"go_id\"].isin(results[\"go_1\"])]\n",
    "        self.plot_kde_with_overlay(\n",
    "            df, top_go_df_conv1, information_accretion_col, label=\"Conv 1\"\n",
    "        )\n",
    "        top_tokens_conv1 = self.plot_tfidf(\n",
    "            top_go_df_conv1, text_col, label=\"Top GO Terms Conv1\", top_n=top_n\n",
    "        )\n",
    "\n",
    "        top_go_df_conv2 = df[df[\"go_id\"].isin(results[\"go_2\"])]\n",
    "        self.plot_kde_with_overlay(\n",
    "            df, top_go_df_conv2, information_accretion_col, label=\"Conv 2\"\n",
    "        )\n",
    "        top_tokens_conv2 = self.plot_tfidf(\n",
    "            top_go_df_conv2, text_col, label=\"Top GO Terms Conv2\", top_n=top_n\n",
    "        )\n",
    "\n",
    "        self.plot_heatmap(\n",
    "            df,\n",
    "            results[\"go_1\"],\n",
    "            results[\"go_2\"],\n",
    "            information_accretion_col,\n",
    "            top_n=top_n,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        return top_tokens_conv1, top_tokens_conv2, top_go_df_conv1, top_go_df_conv2\n",
    "\n",
    "    def analyze_feature_importance(self):\n",
    "        # Implement SHAP analysis here\n",
    "        pass\n",
    "\n",
    "    def visualize_attention_flow(self):\n",
    "        # Implement attention flow visualization\n",
    "        pass\n",
    "\n",
    "    def explain_prediction(self, protein_id):\n",
    "        # Implement individual prediction explanation\n",
    "        pass\n",
    "\n",
    "    def analyze_errors(self):\n",
    "        # Implement error analysis\n",
    "        pass\n",
    "\n",
    "    def test_robustness(self):\n",
    "        # Implement robustness analysis\n",
    "        pass\n",
    "\n",
    "    def layer_wise_relevance_propagation(self):\n",
    "        # Implement LRP\n",
    "        pass\n",
    "\n",
    "    def analyze_graph_structure(self):\n",
    "        # Implement graph structure analysis\n",
    "        pass\n",
    "\n",
    "    def ablation_study(self):\n",
    "        # Implement ablation study functionality\n",
    "        pass\n",
    "\n",
    "    def concept_activation_vectors(self):\n",
    "        # Implement TCAV\n",
    "        pass\n",
    "\n",
    "    def generate_counterfactuals(self):\n",
    "        # Implement counterfactual generation\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Aid2GO Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Prot2GO Dataset\n",
    "filepath = base_dir / \"data/ppi/protgo_dataset.pt\"\n",
    "protgo_dataset = torch.load(filepath)\n",
    "protgo_dataset.to_device(device)\n",
    "protgo_dataset\n",
    "\n",
    "# Define output dim for models\n",
    "out_features = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "config = {\n",
    "    \"lr\": 0.001,\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 100000,\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"heads\": 8,\n",
    "    \"out_features\": out_features,\n",
    "    \"scheduler_patience\": 10,\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"balance\": \"1:2\",\n",
    "}\n",
    "\n",
    "# Model instance\n",
    "model = HGNNGAT(\n",
    "    protein_features=protgo_dataset.hetero_data[\"protein\"].x.size(1),\n",
    "    go_features=protgo_dataset.hetero_data[\"go\"].x.size(1),\n",
    "    out_features=config[\"out_features\"],\n",
    "    dropout=config[\"dropout_rate\"],\n",
    "    heads=config[\"heads\"],\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb logging\n",
    "run = wandb.init(\n",
    "    project=\"prot2go\",\n",
    "    notes=\"Baseline GAT Model\",\n",
    "    tags=[\n",
    "        \"PPI data: human\",\n",
    "        \"balance 1:2\",\n",
    "    ],\n",
    "    config=config,\n",
    "    # mode=\"disabled\"\n",
    ")\n",
    "wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "# Train and save best model\n",
    "save_path =  output_dir / f\"models/baseline_{config[\"out_features\"]}_{config[\"balance\"]}\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train(model=model, dataset=protgo_dataset, config=config, run=run, save_path=save_path, use_scheduler=True,)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "# Instantiate the model\n",
    "model = HGNNGAT(\n",
    "    protein_features=protgo_dataset.hetero_data[\"protein\"].x.size(1),\n",
    "    go_features=protgo_dataset.hetero_data[\"go\"].x.size(1),\n",
    "    out_features=config[\"out_features\"],\n",
    "    dropout=config[\"dropout_rate\"],\n",
    "    heads=config[\"heads\"],\n",
    ")\n",
    "\n",
    "# Load best model's state_dict\n",
    "save_path =  output_dir / f\"models/baseline_{config[\"out_features\"]}_{config[\"balance\"]}\"\n",
    "filepath = save_path / \"best_model_weights.pt\"\n",
    "state_dict = torch.load(filepath)\n",
    "model.load_state_dict(state_dict)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_model_performance(\n",
    "    model,\n",
    "    protgo_dataset.hetero_data,\n",
    "    protgo_dataset.associations_combined_df,\n",
    "    protgo_dataset.go_ids_map,\n",
    "    save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions in CAFA format for evaluation\n",
    "\n",
    "# Create folder to save predictions\n",
    "save_path = output_dir / \"predictions\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "filename = f\"predictions_baseline_{out_features}.tsv\"\n",
    "\n",
    "# Filter columns and round scores (cafaeval accepts only 3 decimals)\n",
    "predictions = results_df[[\"Protein ID\", \"GO ID\", \"Probability\"]]\n",
    "predictions[\"Probability\"] = predictions[\"Probability\"].round(3)\n",
    "\n",
    "# Sort by protein identiiers (optional), reset index and save\n",
    "predictions = predictions.sort_values(by=\"Protein ID\")\n",
    "predictions.reset_index(drop=True, inplace=True)\n",
    "predictions.to_csv(\n",
    "    save_path / filename,\n",
    "    sep=\"\\t\",\n",
    "    header=False,\n",
    "    index=False,\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = Explainability(\n",
    "    model=model, hetero_data=protgo_dataset.hetero_data, save_path=save_path\n",
    ")\n",
    "\n",
    "results, plots = explainer.plot_node_importance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df, plot = explainer.compare_degrees(\n",
    "    results[\"protein_1\"],\n",
    "    \"protein\",\n",
    "    protgo_dataset.hetero_data[\"protein\", \"ppi\", \"protein\"].edge_index,\n",
    ")\n",
    "\n",
    "comparison_df, plot = explainer.compare_degrees(\n",
    "    results[\"protein_2\"],\n",
    "    \"protein\",\n",
    "    protgo_dataset.hetero_data[\"protein\", \"ppi\", \"protein\"].edge_index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df, plot = explainer.compare_degrees(\n",
    "    results[\"go_1\"],\n",
    "    \"go\",\n",
    "    protgo_dataset.hetero_data[\"go\", \"relation\", \"go\"].edge_index,\n",
    ")\n",
    "\n",
    "comparison_df, plot = explainer.compare_degrees(\n",
    "    results[\"go_2\"],\n",
    "    \"go\",\n",
    "    protgo_dataset.hetero_data[\"go\", \"relation\", \"go\"].edge_index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GO dataset\n",
    "go_df = pd.read_csv(base_dir / \"data/go/go-basic.csv\")\n",
    "\n",
    "top_tokens_conv1, top_tokens_conv2, top_go_df_conv1, top_go_df_conv2 = (\n",
    "    explainer.analyze_top_go(\n",
    "        go_df,\n",
    "        results,\n",
    "        information_accretion_col=\"ia\",\n",
    "        text_col=\"definition\",\n",
    "        top_n=25,\n",
    "        random_state=None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focused Attention with Static Node Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "config = {\n",
    "    \"lr\": 0.001,\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 100000,\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"heads\": 8,\n",
    "    \"out_features\": out_features,\n",
    "    \"scheduler_patience\": 10,\n",
    "    \"early_stopping_patience\": 25,\n",
    "    \"protein_sample_size\":10,\n",
    "    \"protein_high_degree_threshold\":10,\n",
    "    \"go_sample_size\":10,\n",
    "    \"go_high_degree_threshold\":20,\n",
    "    \"balance\": \"1:2\",\n",
    "}\n",
    "\n",
    "# Model instance\n",
    "model = FocusedHGNNGATdirect(\n",
    "    protein_features=protgo_dataset.hetero_data[\"protein\"].x.size(1),\n",
    "    go_features=protgo_dataset.hetero_data[\"go\"].x.size(1),\n",
    "    out_features=config[\"out_features\"],\n",
    "    dropout=config[\"dropout_rate\"],\n",
    "    heads=config[\"heads\"],\n",
    "    concat=True,\n",
    "    protein_sample_size=config[\"protein_sample_size\"],\n",
    "    protein_high_degree_threshold=config[\"protein_high_degree_threshold\"],\n",
    "    go_sample_size=config[\"go_sample_size\"],\n",
    "    go_high_degree_threshold=config[\"go_high_degree_threshold\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb logging\n",
    "run = wandb.init(\n",
    "    project=\"prot2go\",\n",
    "    notes=\"Focused GAT Model\",\n",
    "    tags=[\n",
    "        \"human PPI\",\n",
    "        \"balance 1:2\",\n",
    "        \"go def embeddings\",\n",
    "        \"biobert model\",\n",
    "    ],\n",
    "    config=config,\n",
    "    # mode=\"disabled\"\n",
    ")\n",
    "wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "save_path =  output_dir / f\"models/focused_{config[\"out_features\"]}_{config[\"balance\"]}\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train(model=model, dataset=protgo_dataset, config=config, run=run, save_path=save_path, use_scheduler=True,)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "# Model instance\n",
    "model = FocusedHGNNGATdirect(\n",
    "    protein_features=protgo_dataset.hetero_data[\"protein\"].x.size(1),\n",
    "    go_features=protgo_dataset.hetero_data[\"go\"].x.size(1),\n",
    "    out_features=config[\"out_features\"],\n",
    "    dropout=config[\"dropout_rate\"],\n",
    "    heads=config[\"heads\"],\n",
    "    concat=True,\n",
    "    protein_sample_size=config[\"protein_sample_size\"],\n",
    "    protein_high_degree_threshold=config[\"protein_high_degree_threshold\"],\n",
    "    go_sample_size=config[\"go_sample_size\"],\n",
    "    go_high_degree_threshold=config[\"go_high_degree_threshold\"]\n",
    ")\n",
    "\n",
    "# Load best model's state_dict\n",
    "save_path =  output_dir / f\"models/focused_{config[\"out_features\"]}_{config[\"balance\"]}\"\n",
    "filepath = save_path / \"best_model_weights.pt\"\n",
    "state_dict = torch.load(filepath)\n",
    "model.load_state_dict(state_dict)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_model_performance(\n",
    "    model,\n",
    "    protgo_dataset.hetero_data,\n",
    "    protgo_dataset.associations_combined_df,\n",
    "    protgo_dataset.go_ids_map,\n",
    "    save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions in CAFA format for evaluation\n",
    "\n",
    "# Create folder to save predictions\n",
    "save_path = output_dir / \"predictions\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "filename = f\"predictions_focused-attn_{out_features}.tsv\"\n",
    "\n",
    "# Filter columns and round scores (cafaeval accepts only 3 decimals)\n",
    "predictions = results_df[[\"Protein ID\", \"GO ID\", \"Probability\"]]\n",
    "predictions[\"Probability\"] = predictions[\"Probability\"].round(3)\n",
    "\n",
    "# Sort by protein identiiers (optional), reset index and save\n",
    "predictions = predictions.sort_values(by=\"Protein ID\")\n",
    "predictions.reset_index(drop=True, inplace=True)\n",
    "predictions.to_csv(\n",
    "    save_path / filename,\n",
    "    sep=\"\\t\",\n",
    "    header=False,\n",
    "    index=False,\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = Explainability(\n",
    "    model=model, hetero_data=protgo_dataset.hetero_data, save_path=save_path\n",
    ")\n",
    "explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, plots = explainer.plot_node_importance(top_n=25)\n",
    "\n",
    "for plot in plots:\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df, plot = explainer.compare_degrees(\n",
    "    results[\"protein_1\"],\n",
    "    \"protein\",\n",
    "    protgo_dataset.hetero_data[\"protein\", \"ppi\", \"protein\"].edge_index,\n",
    ")\n",
    "\n",
    "comparison_df, plot = explainer.compare_degrees(\n",
    "    results[\"protein_2\"],\n",
    "    \"protein\",\n",
    "    protgo_dataset.hetero_data[\"protein\", \"ppi\", \"protein\"].edge_index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df_1, plot = explainer.compare_degrees(\n",
    "    results[\"go_1\"],\n",
    "    \"go\",\n",
    "    protgo_dataset.hetero_data[\"go\", \"relation\", \"go\"].edge_index,\n",
    ")\n",
    "\n",
    "comparison_df_2, plot = explainer.compare_degrees(\n",
    "    results[\"go_2\"],\n",
    "    \"go\",\n",
    "    protgo_dataset.hetero_data[\"go\", \"relation\", \"go\"].edge_index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_tokens_conv1, top_tokens_conv2, top_go_df_conv1, top_go_df_conv2 = (\n",
    "    explainer.analyze_top_go(\n",
    "        go_df,\n",
    "        results,\n",
    "        information_accretion_col=\"ia\",\n",
    "        text_col=\"definition\",\n",
    "        top_n=25,\n",
    "        random_state=None,\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aid2go",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
